# LiteLLM + Supabase Logging Implementation

This document describes the comprehensive LiteLLM integration with Supabase logging implemented for VT.ai, enabling detailed request tracking, token usage monitoring, and cost analysis across all LLM providers.

## Features

✅ **Multi-Provider Support**: OpenAI, Anthropic, Google, Azure, Cohere, Replicate, PaLM, and more
✅ **Automatic Logging**: Success and failure callbacks log all requests
✅ **Token Tracking**: Per-user token usage with monthly aggregation
✅ **Cost Monitoring**: Real-time cost tracking and spending analysis
✅ **User Relationship**: Links requests to user profiles with proper RLS
✅ **Provider Analytics**: Breakdown by model and provider

## Database Schema

### Enhanced `request_logs` Table

```sql
CREATE TABLE public.request_logs (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    model TEXT DEFAULT '',
    messages JSON DEFAULT '{}',
    response JSON DEFAULT '{}',
    end_user TEXT DEFAULT '',
    status TEXT DEFAULT '',
    error JSON DEFAULT '{}',
    response_time REAL DEFAULT 0,
    total_cost REAL,
    additional_details JSON DEFAULT '{}',
    litellm_call_id TEXT UNIQUE,
    user_profile_id TEXT,
    tokens_used INTEGER DEFAULT 0,
    provider TEXT DEFAULT '',
    PRIMARY KEY (id),
    FOREIGN KEY (user_profile_id) REFERENCES user_profiles(user_id)
);
```

### New `tokens_usage` Table

```sql
CREATE TABLE public.tokens_usage (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY,
    user_profile_id TEXT NOT NULL,
    total_tokens INTEGER DEFAULT 0,
    total_cost REAL DEFAULT 0.0,
    period_start TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    period_end TIMESTAMP WITH TIME ZONE,
    period_type TEXT DEFAULT 'monthly',
    model_breakdown JSON DEFAULT '{}',
    provider_breakdown JSON DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (id),
    FOREIGN KEY (user_profile_id) REFERENCES user_profiles(user_id),
    UNIQUE (user_profile_id, period_type, period_start)
);
```

## Row Level Security (RLS)

Both tables have comprehensive RLS policies:

- **Service Role & Anon**: Full access for system operations
- **Authenticated Users**: Can only access their own data
- **Data Isolation**: Users cannot see other users' requests or usage

## Implementation Components

### 1. Enhanced Supabase Logger (`vtai/utils/supabase_client.py`)

```python
from utils.supabase_client import setup_litellm_callbacks

# Setup LiteLLM callbacks
setup_litellm_callbacks()

# Automatic logging on all LLM calls
response = litellm.completion(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello"}],
    user="user_123"  # Links to user_profiles.user_id
)
```

### 2. LiteLLM Integration Module (`vtai/utils/litellm_integration.py`)

```python
from utils.litellm_integration import get_llm_client

client = get_llm_client()
response = client.chat_completion(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello"}],
    user="user_123"
)
```

### 3. Updated Conversation Handlers

The existing conversation handlers in `vtai/utils/conversation_handlers.py` now include:

- Enhanced token tracking
- Provider detection
- Cost calculation
- Automatic user profile linking

## Usage Examples

### Basic LiteLLM Usage

```python
import litellm
from utils.supabase_client import setup_litellm_callbacks

# Setup callbacks once
setup_litellm_callbacks()

# All subsequent calls are automatically logged
response = litellm.completion(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello"}],
    user="user_123"
)
```

### Multi-Provider Usage

```python
# OpenAI
response = litellm.completion(model="gpt-4o-mini", ...)

# Anthropic
response = litellm.completion(model="claude-3-haiku-20240307", ...)

# Google
response = litellm.completion(model="gemini/gemini-1.5-flash", ...)

# All automatically logged with provider detection
```

### Image Generation

```python
client = get_llm_client()
response = client.image_generation(
    prompt="A serene mountain landscape",
    model="dall-e-3",
    user="user_123"
)
```

## Testing

### Run Logging Tests

```bash
python examples/test_supabase_logging.py
```

### Run Full Example

```bash
python examples/litellm_supabase_standalone.py
```

## Token Usage Analytics

The system provides detailed usage analytics:

### Per-User Tracking

- Real-time token consumption
- Monthly usage aggregation
- Cost breakdown by model/provider
- Historical usage trends

### Monthly Aggregation

```python
# Automatic monthly usage tracking
{
    \"user_profile_id\": \"user_123\",
    \"total_tokens\": 15000,
    \"total_cost\": 0.45,
    \"model_breakdown\": {
        \"gpt-4o-mini\": 12000,
        \"claude-3-haiku\": 3000
    },
    \"provider_breakdown\": {
        \"openai\": 12000,
        \"anthropic\": 3000
    }
}
```

## Migration Applied

The following migrations were applied:

1. `enhance_request_logs_with_tokens_tracking` - Enhanced request_logs table
2. `add_rls_policies_for_enhanced_tables` - RLS policies for both tables
3. `fix_rls_policies_for_anon_access` - Fixed RLS for anon access
4. `create_test_user_and_fix_constraints` - Test user and constraint fixes

## Environment Variables

Required environment variables:

```bash
SUPABASE_URL=your-supabase-url
SUPABASE_ANON_KEY=your-supabase-anon-key

# LLM Provider API Keys
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GOOGLE_API_KEY=your-google-key
# ... other provider keys
```

## Benefits

1. **Unified Logging**: Single interface for all LLM providers
2. **Cost Control**: Real-time spend tracking and alerts
3. **User Analytics**: Per-user usage patterns and optimization
4. **Debugging**: Comprehensive request/response logging
5. **Compliance**: Full audit trail for LLM usage
6. **Performance**: Async logging doesn't impact response times

## Security Features

- **RLS Enforcement**: Users can only access their own data
- **Data Isolation**: Complete separation between user data
- **Audit Trail**: Full logging of all LLM interactions
- **Foreign Key Integrity**: Proper relational data structure

## Next Steps

1. **Dashboard**: Build usage analytics dashboard
2. **Alerts**: Implement cost/usage threshold alerts
3. **Rate Limiting**: Add per-user rate limiting based on usage
4. **Export**: Add usage data export functionality
5. **Caching**: Implement response caching for cost optimization

---

This implementation provides a robust foundation for LLM usage tracking, cost management, and user analytics in the VT.ai application.
