{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VT.ai Documentation","text":"<p>Welcome to the VT.ai documentation. VT.ai is a minimal multimodal AI chat application with dynamic routing capabilities.</p>"},{"location":"#about-vtai","title":"About VT.ai","text":"<p>VT.ai is a sophisticated multimodal AI chat application that integrates multiple AI providers (OpenAI, Anthropic, Google, etc.) with smart semantic routing to direct queries to the most appropriate handlers. It supports text, image, and audio inputs, includes vision analysis for images, and features advanced image generation capabilities with DALL-E 3 and GPT-Image-1.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-Provider AI Integration: Supports OpenAI (o1, o3, 4o), Anthropic (Claude), Google (Gemini), DeepSeek, Meta (Llama), Cohere, local models via Ollama, and more.</li> <li>Semantic-Based Routing: Smart routing system that automatically directs queries to specialized handlers based on vector-based classification.</li> <li>Multimodal Capabilities: Support for text, image, and audio inputs with vision analysis for images and URLs.</li> <li>Advanced Image Generation: Create custom images using DALL-E 3 and GPT-Image-1 with support for transparent backgrounds and multiple output formats.</li> <li>Voice Interaction: Speech-to-text and real-time conversation features with multiple voice models.</li> <li>Thinking Mode: Access step-by-step reasoning from the models with transparent thinking processes.</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This documentation is organized into several sections:</p> <ul> <li>User Guide: Information for end users of VT.ai, including setup and usage instructions.</li> <li>Developer Guide: Information for developers who want to extend or modify VT.ai.</li> <li>API Reference: Detailed API documentation for VT.ai's components.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with VT.ai, see the Getting Started guide.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions to VT.ai and its documentation are welcome. See the GitHub repository for more information.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed reference documentation for VT.ai's components and modules. The documentation is generated directly from the codebase and includes information about classes, methods, functions, and their parameters.</p>"},{"location":"api/#core-components","title":"Core Components","text":"<p>VT.ai is organized into several core components:</p>"},{"location":"api/#app-module","title":"App Module","text":"<p>The App module is the main entry point for the VT.ai application. It handles initialization, manages the Chainlit web interface, and coordinates the flow of conversations.</p> <p>View App Documentation</p>"},{"location":"api/#router-module","title":"Router Module","text":"<p>The Router module contains the semantic routing logic that classifies user queries and directs them to appropriate handlers. It uses FastEmbed to encode queries and match them to predefined intents.</p> <p>View Router Documentation</p>"},{"location":"api/#utils-module","title":"Utils Module","text":"<p>The Utils module provides various utility functions and classes that support the core functionality of VT.ai, including configuration management, conversation handling, and media processing.</p> <p>View Utils Documentation</p>"},{"location":"api/#assistants-module","title":"Assistants Module","text":"<p>The Assistants module implements specialized AI assistants with capabilities like code interpretation, file processing, and function calling.</p> <p>View Assistants Documentation</p>"},{"location":"api/#using-the-api-reference","title":"Using the API Reference","text":"<p>Each page in the API reference includes:</p> <ul> <li>Class and function definitions</li> <li>Parameter descriptions</li> <li>Return value information</li> <li>Usage examples where available</li> <li>Source code links</li> </ul> <p>This documentation is intended for developers who want to understand, extend, or modify VT.ai's functionality.</p>"},{"location":"api/app/","title":"App Module API Reference","text":"<p>This page documents the main application module of VT.ai (<code>vtai/app.py</code>), which serves as the entry point and core controller for the application.</p>"},{"location":"api/app/#overview","title":"Overview","text":"<p>The app module coordinates the entire VT.ai application, handling user interactions, routing queries, processing responses, and managing the Chainlit web interface. It initializes the application, sets up chat profiles, processes user messages, and manages assistant tools.</p>"},{"location":"api/app/#key-functions","title":"Key Functions","text":""},{"location":"api/app/#application-initialization","title":"Application Initialization","text":"<pre><code>route_layer, assistant_id, openai_client, async_openai_client = initialize_app()\n</code></pre> <p>Initializes the application and returns the routing layer, assistant ID, and OpenAI clients.</p>"},{"location":"api/app/#chat-profile-setup","title":"Chat Profile Setup","text":"<pre><code>@cl.set_chat_profiles\nasync def build_chat_profile(_=None):\n    \"\"\"Define and set available chat profiles.\"\"\"\n    # ...\n</code></pre> <p>Defines and sets available chat profiles for the Chainlit interface. This function is decorated with <code>@cl.set_chat_profiles</code> to register it with the Chainlit framework.</p>"},{"location":"api/app/#chat-session-initialization","title":"Chat Session Initialization","text":"<pre><code>@cl.on_chat_start\nasync def start_chat():\n    \"\"\"Initialize the chat session with settings and system message.\"\"\"\n    # ...\n</code></pre> <p>Initializes the chat session when a user starts a new conversation. Sets default settings, builds the LLM profile, and configures the session with the selected model.</p>"},{"location":"api/app/#message-processing","title":"Message Processing","text":"<pre><code>@cl.on_message\nasync def on_message(message: cl.Message) -&gt; None:\n    \"\"\"\n    Handle incoming user messages and route them appropriately.\n\n    Args:\n        message: The user message object\n    \"\"\"\n    # ...\n</code></pre> <p>Processes incoming user messages. Determines whether to use assistant mode or standard chat mode, handles file attachments, and routes the message to the appropriate handler.</p>"},{"location":"api/app/#assistant-run-management","title":"Assistant Run Management","text":"<pre><code>@cl.step(name=APP_NAME, type=\"run\")\nasync def run(thread_id: str, human_query: str, file_ids: Optional[List[str]] = None):\n    \"\"\"\n    Run the assistant with the user query and manage the response.\n\n    Args:\n        thread_id: Thread ID to interact with\n        human_query: User's message\n        file_ids: Optional list of file IDs to attach\n    \"\"\"\n    # ...\n</code></pre> <p>Manages assistant runs when using the assistant mode. Creates a thread if necessary, adds the user message, and processes the assistant's response.</p>"},{"location":"api/app/#tool-call-processing","title":"Tool Call Processing","text":"<pre><code>async def process_tool_calls(\n    step_details: Any, step_references: Dict[str, cl.Step], step: Any\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Process all tool calls from a step.\n\n    Args:\n        step_details: The step details object\n        step_references: Dictionary of step references\n        step: The run step\n\n    Returns:\n        List of tool outputs\n    \"\"\"\n    # ...\n</code></pre> <p>Processes tool calls from the assistant, such as code interpreter, retrieval, and function calls.</p>"},{"location":"api/app/#settings-management","title":"Settings Management","text":"<pre><code>@cl.on_settings_update\nasync def update_settings(settings: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Update user settings based on preferences.\n\n    Args:\n        settings: Dictionary of user settings\n    \"\"\"\n    # ...\n</code></pre> <p>Updates user settings when they are changed in the interface. Handles settings for model selection, temperature, top_p, image generation, TTS, and other options.</p>"},{"location":"api/app/#tts-response-handling","title":"TTS Response Handling","text":"<pre><code>@cl.action_callback(\"speak_chat_response_action\")\nasync def on_speak_chat_response(action: cl.Action) -&gt; None:\n    \"\"\"\n    Handle TTS action triggered by the user.\n\n    Args:\n        action: The action object containing payload\n    \"\"\"\n    # ...\n</code></pre> <p>Handles text-to-speech actions triggered by the user. Converts text responses to speech using the selected TTS model.</p>"},{"location":"api/app/#configuration-setup","title":"Configuration Setup","text":"<pre><code>def setup_chainlit_config():\n    \"\"\"\n    Sets up a centralized Chainlit configuration directory in ~/.config/vtai/.chainlit\n    and creates symbolic links from the current directory to avoid file duplication.\n    This process is fully automated and requires no user intervention.\n\n    Returns:\n        Path: Path to the centralized chainlit config directory\n    \"\"\"\n    # ...\n</code></pre> <p>Sets up the Chainlit configuration directory and creates necessary symbolic links.</p>"},{"location":"api/app/#main-entry-point","title":"Main Entry Point","text":"<pre><code>def main():\n    \"\"\"\n    Entry point for the VT.ai application when installed via pip.\n    This function is called when the 'vtai' command is executed.\n    \"\"\"\n    # ...\n</code></pre> <p>The main entry point for the application. Parses command-line arguments, sets up the environment, and starts the Chainlit server.</p>"},{"location":"api/app/#helper-functions","title":"Helper Functions","text":""},{"location":"api/app/#tool-processing-functions","title":"Tool Processing Functions","text":"<ul> <li><code>process_code_interpreter_tool</code>: Processes code interpreter tool calls</li> <li><code>process_function_tool</code>: Processes function tool calls</li> <li><code>process_retrieval_tool</code>: Processes retrieval tool calls</li> </ul>"},{"location":"api/app/#run-management","title":"Run Management","text":"<ul> <li><code>create_run_instance</code>: Creates a run instance for the assistant</li> <li><code>managed_run_execution</code>: Context manager for safe run execution</li> </ul>"},{"location":"api/app/#usage-examples","title":"Usage Examples","text":""},{"location":"api/app/#starting-the-application","title":"Starting the Application","text":"<pre><code># Standard startup\nmain()\n\n# Or if running as the main script\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"api/app/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Example of customizing the initialization\nfrom vtai.app import setup_chainlit_config, initialize_app\n\n# Setup custom configuration\nconfig_dir = setup_chainlit_config()\n\n# Initialize with custom options\nroute_layer, assistant_id, openai_client, async_openai_client = initialize_app(\n    custom_option=True\n)\n</code></pre>"},{"location":"api/app/#source-code","title":"Source Code","text":"<p>For the complete source code of the app module, see the GitHub repository.</p>"},{"location":"api/assistants/","title":"Assistants API Reference","text":"<p>This page documents the assistants module of VT.ai (<code>vtai/assistants/</code>), which provides specialized AI assistants with enhanced capabilities.</p>"},{"location":"api/assistants/#overview","title":"Overview","text":"<p>The assistants module implements VT.ai's Assistant mode, which uses OpenAI's Assistants API to provide more advanced features like code interpretation, file processing, and function calling. This module manages the creation and execution of assistant threads and runs.</p>"},{"location":"api/assistants/#key-components","title":"Key Components","text":""},{"location":"api/assistants/#assistant-configuration","title":"Assistant Configuration","text":"<p>The Assistant configuration is defined in the application initialization:</p> <pre><code># Create an assistant with tools\nasync def create_assistant(client, name, instructions, tools=None, model=\"o3\"):\n    \"\"\"\n    Create an assistant with specified configuration.\n\n    Args:\n        client: OpenAI client\n        name: Assistant name\n        instructions: System instructions\n        tools: List of tool configurations\n        model: Model to use\n\n    Returns:\n        Assistant object\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/assistants/#thread-management","title":"Thread Management","text":"<p>Threads are used to maintain conversation context in Assistant mode:</p> <pre><code># Create a new thread\nasync def create_thread(client):\n    \"\"\"\n    Create a new conversation thread.\n\n    Args:\n        client: OpenAI client\n\n    Returns:\n        Thread object\n    \"\"\"\n    # ...\n\n# Add a message to a thread\nasync def add_message_to_thread(client, thread_id, content, role=\"user\"):\n    \"\"\"\n    Add a message to a thread.\n\n    Args:\n        client: OpenAI client\n        thread_id: ID of the thread\n        content: Message content\n        role: Message role (user or assistant)\n\n    Returns:\n        Message object\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/assistants/#run-management","title":"Run Management","text":"<p>Runs are used to execute assistant operations:</p> <pre><code># Create a run\nasync def create_run(client, thread_id, assistant_id):\n    \"\"\"\n    Create a run for assistant processing.\n\n    Args:\n        client: OpenAI client\n        thread_id: ID of the thread\n        assistant_id: ID of the assistant\n\n    Returns:\n        Run object\n    \"\"\"\n    # ...\n\n# Poll run status\nasync def poll_run(client, thread_id, run_id):\n    \"\"\"\n    Poll the status of a run.\n\n    Args:\n        client: OpenAI client\n        thread_id: ID of the thread\n        run_id: ID of the run\n\n    Returns:\n        Updated run object\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/assistants/#assistant-tools","title":"Assistant Tools","text":""},{"location":"api/assistants/#code-interpreter-tool","title":"Code Interpreter Tool","text":"<p>The code interpreter tool allows executing Python code:</p> <pre><code># Process code interpreter output\nasync def process_code_interpreter(step_details):\n    \"\"\"\n    Process code interpreter output.\n\n    Args:\n        step_details: Details of the step\n\n    Returns:\n        Processed output\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/assistants/#retrieval-tool","title":"Retrieval Tool","text":"<p>The retrieval tool handles information retrieval:</p> <pre><code># Process retrieval output\nasync def process_retrieval(step_details):\n    \"\"\"\n    Process retrieval output.\n\n    Args:\n        step_details: Details of the step\n\n    Returns:\n        Retrieved information\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/assistants/#function-calling-tool","title":"Function Calling Tool","text":"<p>The function calling tool enables interaction with external systems:</p> <pre><code># Process function call\nasync def process_function_call(step_details):\n    \"\"\"\n    Process function call.\n\n    Args:\n        step_details: Details of the step\n\n    Returns:\n        Function result\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/assistants/#tool-processing","title":"Tool Processing","text":""},{"location":"api/assistants/#tool-call-processing","title":"Tool Call Processing","text":"<pre><code># Process tool calls\nasync def process_tool_calls(step_details):\n    \"\"\"\n    Process all tool calls in a step.\n\n    Args:\n        step_details: Details of the step\n\n    Returns:\n        List of tool outputs\n    \"\"\"\n    # ...\n\n# Submit tool outputs\nasync def submit_tool_outputs(client, thread_id, run_id, tool_outputs):\n    \"\"\"\n    Submit tool outputs back to the run.\n\n    Args:\n        client: OpenAI client\n        thread_id: ID of the thread\n        run_id: ID of the run\n        tool_outputs: List of tool outputs\n\n    Returns:\n        Updated run object\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/assistants/#message-processing","title":"Message Processing","text":"<pre><code># Process thread messages\nasync def process_thread_message(message_references, thread_message, client):\n    \"\"\"\n    Process a message from a thread.\n\n    Args:\n        message_references: Dictionary of message references\n        thread_message: Message from the thread\n        client: OpenAI client\n    \"\"\"\n    # ...\n\n# Process tool call\nasync def process_tool_call(step_references, step, tool_call, name, input, output, show_input=None):\n    \"\"\"\n    Process and display a tool call.\n\n    Args:\n        step_references: Dictionary of step references\n        step: The step containing the tool call\n        tool_call: The tool call object\n        name: Name of the tool\n        input: Input to the tool\n        output: Output from the tool\n        show_input: Format for displaying input\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/assistants/#usage-examples","title":"Usage Examples","text":""},{"location":"api/assistants/#creating-an-assistant-session","title":"Creating an Assistant Session","text":"<pre><code>from vtai.utils.assistant_tools import process_thread_message, process_tool_call\n\n# Setup assistant session\nasync def setup_assistant_session():\n    # Create or get assistant ID\n    if not assistant_id:\n        # Create a new assistant if none exists\n        assistant = await create_assistant(\n            client=async_openai_client,\n            name=\"VT.ai Code Assistant\",\n            instructions=\"You are a helpful code and data analysis assistant\",\n            tools=[{\"type\": \"code_interpreter\"}, {\"type\": \"retrieval\"}],\n            model=\"o3\"\n        )\n        assistant_id = assistant.id\n\n    # Create a thread for the conversation\n    thread = await async_openai_client.beta.threads.create()\n    cl.user_session.set(\"thread\", thread)\n\n    return thread.id\n</code></pre>"},{"location":"api/assistants/#running-an-assistant-query","title":"Running an Assistant Query","text":"<pre><code># Run an assistant query\nasync def run_assistant_query(thread_id, query):\n    # Add the message to the thread\n    await async_openai_client.beta.threads.messages.create(\n        thread_id=thread_id,\n        role=\"user\",\n        content=query,\n    )\n\n    # Create a run\n    run = await async_openai_client.beta.threads.runs.create(\n        thread_id=thread_id,\n        assistant_id=assistant_id,\n    )\n\n    # Track message and step references\n    message_references = {}\n    step_references = {}\n    tool_outputs = []\n\n    # Poll for completion\n    while True:\n        run = await async_openai_client.beta.threads.runs.retrieve(\n            thread_id=thread_id, run_id=run.id\n        )\n\n        # Process steps\n        steps = await async_openai_client.beta.threads.runs.steps.list(\n            thread_id=thread_id, run_id=run.id, order=\"asc\"\n        )\n\n        for step in steps.data:\n            # Process step details\n            # ...\n\n        # Submit tool outputs if needed\n        if run.status == \"requires_action\" and run.required_action.type == \"submit_tool_outputs\":\n            await async_openai_client.beta.threads.runs.submit_tool_outputs(\n                thread_id=thread_id,\n                run_id=run.id,\n                tool_outputs=tool_outputs,\n            )\n\n        # Check if run is complete\n        if run.status in [\"cancelled\", \"failed\", \"completed\", \"expired\"]:\n            break\n\n        # Wait before polling again\n        await asyncio.sleep(1)\n</code></pre>"},{"location":"api/assistants/#best-practices","title":"Best Practices","text":"<p>When working with the Assistants API:</p> <ol> <li>Thread Management:</li> <li>Create a new thread for each conversation</li> <li>Store thread IDs in user sessions</li> <li> <p>Clean up threads when they're no longer needed</p> </li> <li> <p>Error Handling:</p> </li> <li>Implement timeouts for long-running operations</li> <li>Handle API failures gracefully</li> <li> <p>Provide user feedback during processing</p> </li> <li> <p>Tool Processing:</p> </li> <li>Cache results when appropriate</li> <li>Validate inputs before processing</li> <li>Format outputs for user readability</li> </ol>"},{"location":"api/assistants/#source-code","title":"Source Code","text":"<p>For the complete source code of the assistants module, see the GitHub repository.</p>"},{"location":"api/router/","title":"Router API Reference","text":"<p>This page documents the router module of VT.ai (<code>vtai/router/</code>), which provides semantic routing capabilities.</p>"},{"location":"api/router/#overview","title":"Overview","text":"<p>The router module is responsible for analyzing user queries, determining their intent, and routing them to the appropriate handlers. It uses vector embeddings to understand the semantic meaning of queries, making it more flexible and robust than simple keyword matching.</p>"},{"location":"api/router/#key-components","title":"Key Components","text":""},{"location":"api/router/#semanticrouter-class","title":"SemanticRouter Class","text":"<pre><code>class SemanticRouter:\n    \"\"\"\n    A semantic router that uses vector embeddings to route queries to appropriate handlers.\n    \"\"\"\n\n    def __init__(self, routes, embedding_model=\"BAAI/bge-small-en-v1.5\", threshold=0.7):\n        \"\"\"\n        Initialize the semantic router.\n\n        Args:\n            routes: List of Route objects defining routing patterns\n            embedding_model: Model to use for embeddings\n            threshold: Minimum similarity threshold for routing\n        \"\"\"\n        # ...\n\n    async def route(self, query, context=None):\n        \"\"\"\n        Route a query to the appropriate handler.\n\n        Args:\n            query: User query to route\n            context: Optional context dictionary\n\n        Returns:\n            Routing result with handler and metadata\n        \"\"\"\n        # ...\n</code></pre>"},{"location":"api/router/#route-class","title":"Route Class","text":"<pre><code>class Route:\n    \"\"\"\n    Defines a routing destination with intent and handler.\n    \"\"\"\n\n    def __init__(self, name, description, handler, examples=None):\n        \"\"\"\n        Initialize a route.\n\n        Args:\n            name: Name of the intent\n            description: Description of the intent\n            handler: Function to handle the intent\n            examples: Example queries for this intent\n        \"\"\"\n        # ...\n</code></pre>"},{"location":"api/router/#router-configuration","title":"Router Configuration","text":"<p>The router configuration is defined in <code>vtai/router/layers.json</code>, which contains intents and examples:</p> <pre><code>{\n  \"intents\": [\n    {\n      \"name\": \"general_conversation\",\n      \"description\": \"General conversation queries\",\n      \"examples\": [\n        \"Hello, how are you?\",\n        \"What is the capital of France?\",\n        \"Tell me about quantum physics\",\n        \"...\"\n      ]\n    },\n    {\n      \"name\": \"image_generation\",\n      \"description\": \"Requests to generate images\",\n      \"examples\": [\n        \"Generate an image of a mountain landscape\",\n        \"Create a picture of a futuristic city\",\n        \"Draw a cat playing with a ball of yarn\",\n        \"...\"\n      ]\n    },\n    // Additional intents...\n  ]\n}\n</code></pre>"},{"location":"api/router/#usage-examples","title":"Usage Examples","text":""},{"location":"api/router/#basic-routing","title":"Basic Routing","text":"<pre><code># Import the router module\nfrom vtai.router import SemanticRouter, Route\n\n# Define route handlers\nasync def handle_general(message, context):\n    # Handle general conversation\n    pass\n\nasync def handle_image_gen(message, context):\n    # Handle image generation\n    pass\n\n# Create routes\nroutes = [\n    Route(\"general_conversation\", \"General conversation queries\", handle_general),\n    Route(\"image_generation\", \"Requests to generate images\", handle_image_gen),\n]\n\n# Initialize the router\nrouter = SemanticRouter(routes)\n\n# Route a query\nresult = await router.route(\"Generate an image of a sunset\")\n# result will contain a reference to handle_image_gen and metadata\n</code></pre>"},{"location":"api/router/#custom-embedding-model","title":"Custom Embedding Model","text":"<pre><code># Initialize with a custom embedding model\nrouter = SemanticRouter(\n    routes=routes,\n    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    threshold=0.65\n)\n</code></pre>"},{"location":"api/router/#router-trainer","title":"Router Trainer","text":"<p>The router module includes a trainer for updating embeddings:</p> <pre><code># Import the trainer\nfrom vtai.router.trainer import train_router\n\n# Train the router with new examples\ntrain_router(\n    layers_file=\"path/to/layers.json\",\n    output_file=\"path/to/output.json\"\n)\n</code></pre>"},{"location":"api/router/#best-practices","title":"Best Practices","text":"<p>When working with the router:</p> <ol> <li>Adding New Intents:</li> <li>Include diverse examples for each intent</li> <li>Ensure examples are distinct from other intents</li> <li> <p>Use at least 5-10 examples per intent for good coverage</p> </li> <li> <p>Performance Optimization:</p> </li> <li>Cache embeddings for frequently used queries</li> <li>Use smaller embedding models for faster inference</li> <li> <p>Adjust the similarity threshold based on your needs</p> </li> <li> <p>Error Handling:</p> </li> <li>Implement a fallback route for unclassified queries</li> <li>Log routing decisions for analysis</li> <li>Periodically review and refine intent examples</li> </ol>"},{"location":"api/router/#source-code","title":"Source Code","text":"<p>For the complete source code of the router module, see the GitHub repository.</p>"},{"location":"api/utils/","title":"Utils API Reference","text":"<p>This page documents the utils module of VT (<code>vtai/utils/</code>), which provides various utility functions and classes that support the core functionality of the application.</p>"},{"location":"api/utils/#overview","title":"Overview","text":"<p>The utils module contains helper functions and classes for configuration management, conversation handling, media processing, error handling, and more. These utilities form the foundation of VT's functionality and are used throughout the application.</p>"},{"location":"api/utils/#key-components","title":"Key Components","text":""},{"location":"api/utils/#configuration-management","title":"Configuration Management","text":""},{"location":"api/utils/#configpy","title":"<code>config.py</code>","text":"<pre><code># Initialize the application\ndef initialize_app(custom_option=False):\n    \"\"\"\n    Initialize the VT.ai application.\n\n    Args:\n        custom_option: Optional custom configuration flag\n\n    Returns:\n        Tuple containing the route layer, assistant ID, and OpenAI clients\n    \"\"\"\n    # ...\n\n# Get logger instance\ndef get_logger():\n    \"\"\"\n    Get the application logger.\n\n    Returns:\n        Logger instance configured for VT.ai\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#constantspy","title":"<code>constants.py</code>","text":"<pre><code># Application constants\nAPP_NAME = \"VT\"\nDEFAULT_MODEL = \"o3-mini\"\nDEFAULT_TEMPERATURE = 0.7\nDEFAULT_TOP_P = 0.9\n# ...\n</code></pre>"},{"location":"api/utils/#llm_providers_configpy","title":"<code>llm_providers_config.py</code>","text":"<pre><code># Provider configuration\nPROVIDERS = {\n    \"openai\": {\n        \"name\": \"OpenAI\",\n        \"models\": [\"o1\", \"o3-mini\", \"4o\"],\n        \"env_var\": \"OPENAI_API_KEY\",\n        \"icon\": \"chatgpt-icon.png\",\n    },\n    # Additional providers...\n}\n\n# Model to provider mapping\nMODEL_PROVIDER_MAP = {\n    \"o1\": \"openai\",\n    \"o3-mini\": \"openai\",\n    \"4o\": \"openai\",\n    # Additional mappings...\n}\n\n# Get list of models\ndef get_available_models():\n    \"\"\"\n    Get a list of all available models.\n\n    Returns:\n        List of model names\n    \"\"\"\n    # ...\n\n# Check if a model is a reasoning model\ndef is_reasoning_model(model):\n    \"\"\"\n    Check if a model is categorized as a reasoning model.\n\n    Args:\n        model: Model name to check\n\n    Returns:\n        True if the model is a reasoning model, False otherwise\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#conversation-handlers","title":"Conversation Handlers","text":""},{"location":"api/utils/#conversation_handlerspy","title":"<code>conversation_handlers.py</code>","text":"<pre><code># Handle standard conversation\nasync def handle_conversation(message, messages, route_layer):\n    \"\"\"\n    Handle a standard conversation message.\n\n    Args:\n        message: The user message\n        messages: Message history\n        route_layer: Semantic routing layer\n    \"\"\"\n    # ...\n\n# Handle thinking mode conversation\nasync def handle_thinking_conversation(message, messages, route_layer):\n    \"\"\"\n    Handle a thinking mode conversation message.\n\n    Args:\n        message: The user message\n        messages: Message history\n        route_layer: Semantic routing layer\n    \"\"\"\n    # ...\n\n# Configure chat session\nasync def config_chat_session(settings):\n    \"\"\"\n    Configure the chat session with settings.\n\n    Args:\n        settings: Chat settings dictionary\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#media-processors","title":"Media Processors","text":""},{"location":"api/utils/#media_processorspy","title":"<code>media_processors.py</code>","text":"<pre><code># Handle vision tasks\nasync def handle_vision(message, messages, client):\n    \"\"\"\n    Handle vision analysis for images.\n\n    Args:\n        message: The user message\n        messages: Message history\n        client: LLM client\n    \"\"\"\n    # ...\n\n# Handle image generation\nasync def handle_trigger_async_image_gen(message, messages, client, **kwargs):\n    \"\"\"\n    Handle image generation requests.\n\n    Args:\n        message: The user message\n        messages: Message history\n        client: LLM client\n        **kwargs: Additional arguments\n    \"\"\"\n    # ...\n\n# Handle TTS responses\nasync def handle_tts_response(text, client):\n    \"\"\"\n    Handle text-to-speech response generation.\n\n    Args:\n        text: Text to convert to speech\n        client: OpenAI client\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#file-handlers","title":"File Handlers","text":""},{"location":"api/utils/#file_handlerspy","title":"<code>file_handlers.py</code>","text":"<pre><code># Process files\nasync def process_files(elements, client):\n    \"\"\"\n    Process uploaded files.\n\n    Args:\n        elements: Message elements containing files\n        client: OpenAI client\n\n    Returns:\n        List of file IDs\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#error-handlers","title":"Error Handlers","text":""},{"location":"api/utils/#error_handlerspy","title":"<code>error_handlers.py</code>","text":"<pre><code># Handle exceptions\nasync def handle_exception(exception):\n    \"\"\"\n    Handle an exception and display appropriate error message.\n\n    Args:\n        exception: The exception to handle\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#ui-components","title":"UI Components","text":""},{"location":"api/utils/#llm_profile_builderpy","title":"<code>llm_profile_builder.py</code>","text":"<pre><code># Build LLM profile\ndef build_llm_profile(icons_map):\n    \"\"\"\n    Build the LLM profile with icons.\n\n    Args:\n        icons_map: Mapping of providers to icons\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#settings_builderpy","title":"<code>settings_builder.py</code>","text":"<pre><code># Build settings\nasync def build_settings():\n    \"\"\"\n    Build the settings UI.\n\n    Returns:\n        Settings components\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#helper-utilities","title":"Helper Utilities","text":""},{"location":"api/utils/#dict_to_objectpy","title":"<code>dict_to_object.py</code>","text":"<pre><code>class DictToObject:\n    \"\"\"\n    Convert a dictionary to an object with attributes.\n    \"\"\"\n\n    def __init__(self, data):\n        \"\"\"\n        Initialize with dictionary data.\n\n        Args:\n            data: Dictionary to convert\n        \"\"\"\n        # ...\n</code></pre>"},{"location":"api/utils/#user_session_helperpy","title":"<code>user_session_helper.py</code>","text":"<pre><code># Get setting value\ndef get_setting(key, default=None):\n    \"\"\"\n    Get a setting value from user session.\n\n    Args:\n        key: Setting key\n        default: Default value if not found\n\n    Returns:\n        Setting value\n    \"\"\"\n    # ...\n\n# Check if in assistant profile\ndef is_in_assistant_profile():\n    \"\"\"\n    Check if the current session is using the assistant profile.\n\n    Returns:\n        True if in assistant profile, False otherwise\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"api/utils/#usage-examples","title":"Usage Examples","text":""},{"location":"api/utils/#configuration-example","title":"Configuration Example","text":"<pre><code>from vtai.utils import constants, config\nfrom vtai.utils.llm_providers_config import get_available_models\n\n# Get application logger\nlogger = config.get_logger()\n\n# Get available models\nmodels = get_available_models()\nlogger.info(\"Available models: %s\", models)\n\n# Initialize the application\nroute_layer, assistant_id, openai_client, async_openai_client = config.initialize_app()\n</code></pre>"},{"location":"api/utils/#conversation-handling-example","title":"Conversation Handling Example","text":"<pre><code>from vtai.utils.conversation_handlers import handle_conversation, handle_thinking_conversation\n\n# Handle standard query\nawait handle_conversation(message, message_history, route_layer)\n\n# Handle thinking mode query\nif \"&lt;think&gt;\" in message.content:\n    await handle_thinking_conversation(message, message_history, route_layer)\n</code></pre>"},{"location":"api/utils/#media-processing-example","title":"Media Processing Example","text":"<pre><code>from vtai.utils.media_processors import handle_vision, handle_tts_response\n\n# Process an image\nif message.elements and message.elements[0].type == \"image\":\n    await handle_vision(message, message_history, client)\n\n# Generate speech\nawait handle_tts_response(\"Text to convert to speech\", openai_client)\n</code></pre>"},{"location":"api/utils/#best-practices","title":"Best Practices","text":"<p>When working with the utils module:</p> <ol> <li>Error Handling:</li> <li>Use the <code>handle_exception</code> function for consistent error handling</li> <li> <p>Wrap async operations in try/except blocks</p> </li> <li> <p>Configuration:</p> </li> <li>Access constants from the <code>constants</code> module</li> <li> <p>Use the provider configuration for model operations</p> </li> <li> <p>Session Management:</p> </li> <li>Use the <code>user_session_helper</code> functions to access session data</li> <li>Store persistent data in user sessions</li> </ol>"},{"location":"api/utils/#source-code","title":"Source Code","text":"<p>For the complete source code of the utils module, see the GitHub repository.</p>"},{"location":"developer/architecture/","title":"VT.ai Architecture","text":"<p>This document provides an overview of VT.ai's architecture, explaining how its components work together to create a powerful multimodal AI chat application.</p>"},{"location":"developer/architecture/#implementation-options","title":"Implementation Options","text":"<p>VT.ai is available in two implementations:</p> <ol> <li>Python Implementation: The original implementation built with Python, Chainlit, and LiteLLM</li> <li>Rust Implementation: A high-performance port focused on efficiency and reliability</li> </ol> <p>This document covers both implementations, highlighting their architectural differences and similarities.</p>"},{"location":"developer/architecture/#python-implementation","title":"Python Implementation","text":""},{"location":"developer/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<p>The Python implementation follows a modular architecture designed for flexibility and extensibility:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Web Interface   \u2502     \u2502 Semantic      \u2502     \u2502 Model          \u2502\n\u2502 (Chainlit)      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Router        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Providers      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 File/Media      \u2502     \u2502 Assistant     \u2502     \u2502 API Key        \u2502\n\u2502 Processing      \u2502     \u2502 Tools         \u2502     \u2502 Management     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"developer/architecture/#core-components","title":"Core Components","text":""},{"location":"developer/architecture/#1-entry-point-vtaiapppy","title":"1. Entry Point (<code>vtai/app.py</code>)","text":"<p>The main application entry point handles initialization, manages the Chainlit web interface, and coordinates the conversation flow. It:</p> <ul> <li>Initializes the application configuration</li> <li>Sets up chat profiles and user sessions</li> <li>Manages message handling and routing</li> <li>Processes assistant runs and tool calls</li> <li>Handles user settings and TTS features</li> </ul>"},{"location":"developer/architecture/#2-routing-layer-vtairouter","title":"2. Routing Layer (<code>vtai/router/</code>)","text":"<p>The semantic routing system classifies user queries and directs them to appropriate handlers based on intent:</p> <ul> <li>Encoder: Uses FastEmbed with the BAAI/bge-small-en-v1.5 embedding model</li> <li>Classification: Performs vector similarity matching against predefined intents</li> <li>Dynamic Dispatch: Routes queries to specialized conversation handlers</li> </ul>"},{"location":"developer/architecture/#3-model-management","title":"3. Model Management","text":"<p>The Python implementation uses LiteLLM as a unified interface to multiple AI providers:</p> <ul> <li>Provider Abstraction: Standardizes API calls across different model providers</li> <li>Model Switching: Allows seamless switching between models based on query needs</li> <li>Error Handling: Provides consistent error handling across providers</li> </ul>"},{"location":"developer/architecture/#4-utility-modules-vtaiutils","title":"4. Utility Modules (<code>vtai/utils/</code>)","text":"<p>Various utility modules provide supporting functionality:</p> <ul> <li>Configuration Management: Handles API keys, settings, and environment variables</li> <li>Conversation Handlers: Processes different types of conversations (standard, thinking mode)</li> <li>Media Processors: Handles image analysis, generation, and TTS features</li> <li>File Handlers: Manages file uploads and processing</li> <li>Error Handlers: Provides consistent error handling and reporting</li> </ul>"},{"location":"developer/architecture/#5-assistant-tools-vtaiassistants","title":"5. Assistant Tools (<code>vtai/assistants/</code>)","text":"<p>Implements specialized assistant capabilities:</p> <ul> <li>Code Interpreter: Executes Python code for data analysis</li> <li>Thread Management: Handles persistent conversation threads</li> <li>Tool Processing: Manages function calls and tool outputs</li> </ul>"},{"location":"developer/architecture/#rust-implementation","title":"Rust Implementation","text":""},{"location":"developer/architecture/#high-level-architecture_1","title":"High-Level Architecture","text":"<p>The Rust implementation follows a similar architecture but with performance-focused components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Web Server      \u2502     \u2502 Semantic      \u2502     \u2502 Model          \u2502\n\u2502 (Axum)          \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Router        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Providers      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502                     \u2502\n        \u25bc                       \u25bc                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 WebSocket       \u2502     \u2502 Tool          \u2502     \u2502 Configuration  \u2502\n\u2502 Handlers        \u2502     \u2502 Registry      \u2502     \u2502 Management     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"developer/architecture/#core-components_1","title":"Core Components","text":""},{"location":"developer/architecture/#1-web-server-srcapp","title":"1. Web Server (<code>src/app/</code>)","text":"<p>The entry point for the Rust implementation is based on the Axum web framework:</p> <ul> <li>HTTP Server: Handles REST API endpoints</li> <li>WebSocket Server: Manages real-time chat communication</li> <li>Static Files: Serves the web interface assets</li> <li>API Endpoints: Exposes model selection and configuration endpoints</li> </ul>"},{"location":"developer/architecture/#2-routing-system-srcrouter","title":"2. Routing System (<code>src/router/</code>)","text":"<p>The Rust implementation includes a semantic routing system similar to the Python version:</p> <ul> <li>Intent Classification: Uses embeddings for query classification</li> <li>Router Registry: Manages available routers and their capabilities</li> <li>Dynamic Dispatch: Routes requests to appropriate handlers</li> </ul>"},{"location":"developer/architecture/#3-tool-integration-srctools","title":"3. Tool Integration (<code>src/tools/</code>)","text":"<p>The Rust implementation provides a robust tool integration system:</p> <ul> <li>Tool Registry: Central registry for available tools</li> <li>Code Execution: Safely runs code in isolated environments</li> <li>File Operations: Handles file uploads and processing</li> <li>Search Tools: Provides search capabilities within conversations</li> </ul>"},{"location":"developer/architecture/#4-assistant-integration-srcassistants","title":"4. Assistant Integration (<code>src/assistants/</code>)","text":"<p>Implements the OpenAI Assistant API integration:</p> <ul> <li>Assistant Management: Creates and manages assistants</li> <li>Thread Handling: Manages conversation threads</li> <li>Tool Invocation: Handles tool calls from the assistant</li> </ul>"},{"location":"developer/architecture/#5-utilities-srcutils","title":"5. Utilities (<code>src/utils/</code>)","text":"<p>Common utilities shared across the application:</p> <ul> <li>Error Handling: Comprehensive error types and handling</li> <li>Configuration: Environment-based configuration</li> <li>Model Management: Model alias resolution and provider abstraction</li> <li>Authentication: API key management and validation</li> </ul>"},{"location":"developer/architecture/#data-flow-comparison","title":"Data Flow Comparison","text":"<p>Both implementations follow a similar data flow pattern with some implementation-specific differences:</p>"},{"location":"developer/architecture/#python-implementation_1","title":"Python Implementation","text":"<ol> <li>User Input: User submits a message through the Chainlit web interface</li> <li>Message Processing: The message is processed to extract text and media</li> <li>Intent Classification: The semantic router classifies the query intent</li> <li>Model Selection: A specific model is selected based on classification</li> <li>Query Execution: The query is sent to the appropriate model provider via LiteLLM</li> <li>Response Processing: The response is processed (may include media generation)</li> <li>UI Rendering: The formatted response is displayed in the web interface</li> </ol>"},{"location":"developer/architecture/#rust-implementation_1","title":"Rust Implementation","text":"<ol> <li>User Input: User submits a message through the web interface over WebSocket</li> <li>Message Processing: The WebSocket handler processes the incoming message</li> <li>Intent Classification: The router classifies the message intent</li> <li>Model Selection: The appropriate model is selected based on the classification</li> <li>Query Execution: The query is sent directly to the model provider API</li> <li>Response Streaming: The response is streamed back to the client via WebSocket</li> <li>UI Rendering: The client-side JavaScript renders the response in the interface</li> </ol>"},{"location":"developer/architecture/#configuration-system","title":"Configuration System","text":"<p>Both implementations use a similar configuration approach with some implementation differences:</p>"},{"location":"developer/architecture/#python-implementation_2","title":"Python Implementation","text":"<p>Uses a layered configuration system with priorities:</p> <ol> <li>Command-line arguments: Highest priority</li> <li>Environment variables: Secondary priority</li> <li>Configuration files: <code>~/.config/vtai/.env</code> for persistence</li> <li>Default values: Used when no specific settings are provided</li> </ol>"},{"location":"developer/architecture/#rust-implementation_2","title":"Rust Implementation","text":"<p>Uses a Rust-native configuration approach:</p> <ol> <li>Command-line arguments: Processed using Clap</li> <li>Environment variables: Read using the dotenv crate</li> <li>Configuration files: Similar structure to Python implementation</li> <li>Default values: Defined using Rust's Option and Default traits</li> </ol>"},{"location":"developer/architecture/#extension-points","title":"Extension Points","text":"<p>Both implementations provide extension points for customization:</p>"},{"location":"developer/architecture/#python-implementation_3","title":"Python Implementation","text":"<ol> <li>New Model Providers: Add support for additional AI providers</li> <li>Custom Intents: Extend the semantic router with new intent classifications</li> <li>Specialized Handlers: Create custom handlers for specific conversation types</li> <li>New Assistant Tools: Add new tools for specialized tasks</li> </ol>"},{"location":"developer/architecture/#rust-implementation_3","title":"Rust Implementation","text":"<ol> <li>New Model Providers: Implement new provider traits</li> <li>Custom Routers: Add new router implementations to the registry</li> <li>Tool Extensions: Implement the Tool trait for new functionality</li> <li>Middleware Components: Add new middleware for request/response processing</li> </ol> <p>For details on extending either implementation, see the Extending VT.ai guide.</p>"},{"location":"developer/extending/","title":"Extending VT.ai","text":"<p>This guide explains how to extend VT.ai with new capabilities. VT.ai is designed to be modular and extensible, allowing developers to add new features, models, and integrations.</p>"},{"location":"developer/extending/#adding-new-model-providers","title":"Adding New Model Providers","text":"<p>VT.ai uses LiteLLM to abstract model providers, making it relatively straightforward to add support for new AI models.</p>"},{"location":"developer/extending/#steps-to-add-a-new-provider","title":"Steps to Add a New Provider","text":"<ol> <li>Update Provider Configuration</li> </ol> <p>Edit <code>vtai/utils/llm_providers_config.py</code> to add your new provider:</p> <pre><code># Add to the PROVIDERS dictionary\nPROVIDERS = {\n    # ...existing providers...\n    \"new_provider\": {\n        \"name\": \"New Provider\",\n        \"models\": [\"new-model-1\", \"new-model-2\"],\n        \"env_var\": \"NEW_PROVIDER_API_KEY\",\n        \"icon\": \"new_provider.png\",  # Add icon to vtai/resources/\n    }\n}\n</code></pre> <ol> <li>Add Provider Icon</li> </ol> <p>Place the provider's icon in <code>vtai/resources/</code> directory.</p> <ol> <li>Update Model Mappings</li> </ol> <p>Update the model mappings in the same file to include your new models:</p> <pre><code># Add to MODEL_PROVIDER_MAP\nMODEL_PROVIDER_MAP.update({\n    \"new-model-1\": \"new_provider\",\n    \"new-model-2\": \"new_provider\",\n})\n</code></pre> <ol> <li>Implement Special Handling (if needed)</li> </ol> <p>If your provider requires special handling, add custom logic to the appropriate utility files:</p> <ul> <li>For conversation handling: <code>vtai/utils/conversation_handlers.py</code></li> <li>For media processing: <code>vtai/utils/media_processors.py</code></li> </ul>"},{"location":"developer/extending/#extending-the-semantic-router","title":"Extending the Semantic Router","text":"<p>The semantic router classifies user queries to determine the best handler. You can add new intents to the router for specialized handling.</p>"},{"location":"developer/extending/#steps-to-add-a-new-intent","title":"Steps to Add a New Intent","text":"<ol> <li>Update Routing Layers</li> </ol> <p>Edit <code>vtai/router/layers.json</code> to add your new intent:</p> <pre><code>{\n  \"intents\": [\n    // ...existing intents...\n    {\n      \"name\": \"my_new_intent\",\n      \"description\": \"Description of what this intent handles\",\n      \"examples\": [\n        \"Example query 1\",\n        \"Example query 2\",\n        \"Example query 3\"\n      ]\n    }\n  ]\n}\n</code></pre> <ol> <li>Train the Router</li> </ol> <p>Run the router trainer to update the embeddings:</p> <pre><code>python -m vtai.router.trainer\n</code></pre> <ol> <li>Add Handler Function</li> </ol> <p>Create a handler function in <code>vtai/utils/conversation_handlers.py</code>:</p> <pre><code>async def handle_my_new_intent(message, messages, client, **kwargs):\n    \"\"\"\n    Handle queries matching my_new_intent.\n\n    Args:\n        message: The user message\n        messages: Message history\n        client: LLM client\n        **kwargs: Additional arguments\n    \"\"\"\n    # Your handler implementation here\n    # ...\n\n    # Send response\n    await cl.Message(content=response).send()\n</code></pre> <ol> <li>Update Router Configuration</li> </ol> <p>Add your handler to the routing configuration in the app initialization:</p> <pre><code># In vtai/utils/config.py\nroute_layer = SemanticRouter(\n    # ...existing configuration...\n    routes=[\n        # ...existing routes...\n        Route(\n            name=\"my_new_intent\",\n            description=\"Handles my new intent\",\n            handler=handle_my_new_intent,\n        ),\n    ]\n)\n</code></pre>"},{"location":"developer/extending/#adding-new-assistant-tools","title":"Adding New Assistant Tools","text":"<p>You can extend VT.ai with new assistant tools for specialized capabilities.</p>"},{"location":"developer/extending/#steps-to-add-a-new-tool","title":"Steps to Add a New Tool","text":"<ol> <li>Define Tool Interface</li> </ol> <p>Create a new tool definition in <code>vtai/tools/</code> or extend an existing one:</p> <pre><code># In vtai/tools/my_new_tool.py\nfrom typing import Dict, Any\n\nasync def my_new_tool_function(args: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Implement a new tool capability.\n\n    Args:\n        args: Tool arguments\n\n    Returns:\n        Tool results\n    \"\"\"\n    # Tool implementation\n    # ...\n\n    return {\"result\": \"Output from the tool\"}\n</code></pre> <ol> <li>Register the Tool</li> </ol> <p>Add your tool to the assistant configuration in <code>vtai/utils/assistant_tools.py</code>:</p> <pre><code># Add to the tools list\nASSISTANT_TOOLS = [\n    # ...existing tools...\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"my_new_tool\",\n            \"description\": \"Description of what this tool does\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"param1\": {\n                        \"type\": \"string\",\n                        \"description\": \"Description of param1\"\n                    },\n                    # Add more parameters as needed\n                },\n                \"required\": [\"param1\"]\n            }\n        }\n    }\n]\n</code></pre> <ol> <li>Implement Tool Processing</li> </ol> <p>Add a tool processor in <code>vtai/app.py</code> to handle the tool execution:</p> <pre><code># Add to the process_function_tool logic to handle your tool\nif function_name == \"my_new_tool\":\n    from vtai.tools.my_new_tool import my_new_tool_function\n    result = await my_new_tool_function(function_args)\n    return {\n        \"output\": result,\n        \"tool_call_id\": tool_call.id\n    }\n</code></pre>"},{"location":"developer/extending/#extending-image-generation","title":"Extending Image Generation","text":"<p>VT.ai now uses GPT-Image-1 for image generation with enhanced capabilities and configuration options. Here's how to extend or customize the image generation functionality:</p>"},{"location":"developer/extending/#customizing-image-generation-settings","title":"Customizing Image Generation Settings","text":"<p>You can modify the default settings for GPT-Image-1 by updating the configuration in <code>vtai/utils/llm_providers_config.py</code>:</p> <pre><code># Example: Changing default image generation settings\nDEFAULT_IMAGE_GEN_IMAGE_SIZE = \"1536x1024\"  # Default to landscape format\nDEFAULT_IMAGE_GEN_BACKGROUND = \"transparent\"  # Default to transparent backgrounds\nDEFAULT_IMAGE_GEN_OUTPUT_FORMAT = \"png\"  # Default to PNG format\nDEFAULT_IMAGE_GEN_MODERATION = \"auto\"  # Default moderation level\nDEFAULT_IMAGE_GEN_OUTPUT_COMPRESSION = 90  # Higher quality default\n</code></pre>"},{"location":"developer/extending/#extending-image-processing-features","title":"Extending Image Processing Features","text":"<p>The image generation process is handled in <code>vtai/utils/media_processors.py</code>. To extend this functionality:</p> <ol> <li>Add New Image Processing Features</li> </ol> <p>You can add post-processing features for generated images:</p> <pre><code>async def process_generated_image(image_data, format=\"jpeg\"):\n    \"\"\"\n    Apply custom processing to generated images.\n\n    Args:\n        image_data: Raw image data\n        format: Image format (jpeg, png, webp)\n\n    Returns:\n        Processed image data\n    \"\"\"\n    from PIL import Image, ImageFilter\n    import io\n\n    # Convert bytes to PIL Image\n    image = Image.open(io.BytesIO(image_data))\n\n    # Apply custom processing\n    image = image.filter(ImageFilter.SHARPEN)\n\n    # Convert back to bytes\n    buffer = io.BytesIO()\n    image.save(buffer, format=format.upper())\n\n    return buffer.getvalue()\n</code></pre> <ol> <li>Modify Image Generation UI</li> </ol> <p>To add custom UI elements for your new image settings, update the settings in <code>vtai/utils/settings_builder.py</code>:</p> <pre><code># Add a new setting for image generation\nsettings_components.append(\n    cl.Select(\n        id=\"my_custom_image_setting\",\n        label=\"\ud83d\uddbc\ufe0f My Custom Setting\",\n        values=[\"option1\", \"option2\", \"option3\"],\n        initial_value=\"option1\"\n    )\n)\n</code></pre> <ol> <li>Add Custom Image Metadata</li> </ol> <p>You can modify how image metadata is displayed by updating the <code>handle_trigger_async_image_gen</code> function:</p> <pre><code># Add custom metadata to the image display\nmetadata = {\n    # ...existing metadata...\n    \"Custom Info\": \"Your custom information\",\n    \"Processing\": f\"Applied {your_custom_process}\"\n}\n</code></pre>"},{"location":"developer/extending/#saving-and-managing-generated-images","title":"Saving and Managing Generated Images","text":"<p>Generated images are now saved in an <code>imgs</code> directory with timestamped filenames. To extend this functionality:</p> <ol> <li>Custom Storage Locations</li> </ol> <p>You can modify where images are stored:</p> <pre><code># Example: Change image storage location or naming convention\ntimestamp = int(time.time())\nimg_dir = Path(\"custom_images_folder\")\nimg_dir.mkdir(exist_ok=True)\nimg_path = img_dir / f\"custom_prefix_{timestamp}.{format}\"\n</code></pre> <ol> <li>Image Organization Features</li> </ol> <p>You could add features to organize images by prompt or category:</p> <pre><code># Organize images by category derived from prompt\ndef get_category_from_prompt(prompt):\n    # Simple category extraction\n    if \"landscape\" in prompt.lower():\n        return \"landscapes\"\n    elif \"portrait\" in prompt.lower():\n        return \"portraits\"\n    return \"miscellaneous\"\n\ncategory = get_category_from_prompt(query)\nimg_dir = Path(f\"imgs/{category}\")\nimg_dir.mkdir(exist_ok=True, parents=True)\n</code></pre>"},{"location":"developer/extending/#creating-custom-ui-components","title":"Creating Custom UI Components","text":"<p>VT.ai uses Chainlit for the web interface. You can extend the UI with custom components.</p>"},{"location":"developer/extending/#steps-to-add-custom-ui-elements","title":"Steps to Add Custom UI Elements","text":"<ol> <li>Define Custom Component</li> </ol> <p>Create a custom component in a new file in the <code>vtai</code> directory:</p> <pre><code># In vtai/ui/custom_component.py\nimport chainlit as cl\n\nasync def create_custom_component(data):\n    \"\"\"Create a custom UI component.\"\"\"\n    component = cl.Component(\n        name=\"custom_component\",\n        props={\"data\": data},\n        url=\"custom_component.js\"  # Will need to create this\n    )\n    await component.send()\n</code></pre> <ol> <li>Add Client-Side Implementation</li> </ol> <p>If needed, create JavaScript for client-side functionality in <code>.chainlit/public/custom_component.js</code>.</p> <ol> <li>Integrate Component</li> </ol> <p>Use your component in the appropriate part of the application, such as in conversation handlers.</p>"},{"location":"developer/extending/#testing-extensions","title":"Testing Extensions","text":"<p>To test your extensions:</p> <ol> <li>Run Unit Tests</li> </ol> <p>Create tests for your new functionality in the <code>tests</code> directory:</p> <pre><code># Run tests for your extension\npytest tests/unit/test_my_extension.py\n</code></pre> <ol> <li>Manual Testing</li> </ol> <p>Run VT.ai in development mode to test your changes interactively:</p> <pre><code>chainlit run vtai/app.py -w\n</code></pre>"},{"location":"developer/extending/#best-practices","title":"Best Practices","text":"<p>When extending VT.ai, follow these best practices:</p> <ol> <li>Maintain Backward Compatibility: Ensure your changes don't break existing functionality</li> <li>Follow the Existing Pattern: Maintain the project's coding style and architecture</li> <li>Add Tests: Include tests for your new functionality</li> <li>Update Documentation: Document your extensions in the codebase and in the docs directory</li> <li>Handle Errors Gracefully: Implement proper error handling for your new features</li> <li>Optimize Performance: Consider the performance impact of your changes</li> </ol>"},{"location":"developer/mcp/","title":"Model Context Protocol (MCP) Integration","text":"<p>VT.ai now includes built-in support for the Model Context Protocol (MCP), a standardized way to interact with language models across different providers. This document explains how to use the MCP integration in your VT.ai applications.</p>"},{"location":"developer/mcp/#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) is a standardized protocol for AI model providers and clients to exchange information. It helps standardize how models receive inputs and return outputs, making it easier to swap models or use multiple models in a system.</p> <p>MCP provides several benefits:</p> <ul> <li>Model Interchangeability: Switch easily between models without changing your code</li> <li>Standardized Interface: Consistent API for all language models</li> <li>Improved Stability: Reduced dependency on provider-specific API changes</li> </ul>"},{"location":"developer/mcp/#how-mcp-works-in-vtai","title":"How MCP Works in VT.ai","text":"<p>When you start VT.ai, an MCP server is automatically launched in the background. This server acts as a middleware between your application and various language models, providing a standardized interface.</p> <p>The MCP server:</p> <ol> <li>Accepts requests in a standardized format</li> <li>Maps standardized model names to provider-specific models</li> <li>Forwards the request to the appropriate provider via LiteLLM</li> <li>Returns responses in a standardized format</li> </ol>"},{"location":"developer/mcp/#using-mcp-in-your-applications","title":"Using MCP in Your Applications","text":""},{"location":"developer/mcp/#basic-usage","title":"Basic Usage","text":"<p>You can use MCP in your VT.ai applications by importing the necessary components:</p> <pre><code>from vtai.utils.mcp_integration import create_mcp_completion, initialize_mcp\n\n# Initialize MCP configuration\nmcp_config = initialize_mcp()\n\n# Create a completion\nasync def example_completion():\n    messages = [\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ]\n\n    response = await create_mcp_completion(\n        messages=messages,\n        model=\"gpt-4o-mini\",  # This gets mapped to the actual provider model\n        temperature=0.7\n    )\n\n    content = response.choices[0].message.content\n    print(content)\n</code></pre>"},{"location":"developer/mcp/#streaming-responses","title":"Streaming Responses","text":"<p>MCP supports streaming responses for a better user experience:</p> <pre><code>from vtai.utils.mcp_integration import create_mcp_completion\n\nasync def example_streaming():\n    messages = [\n        {\"role\": \"user\", \"content\": \"Write a short poem about AI\"}\n    ]\n\n    # Define a callback for streaming\n    def stream_callback(token):\n        print(token, end=\"\", flush=True)\n\n    # Stream the response\n    stream = await create_mcp_completion(\n        messages=messages,\n        model=\"gpt-4\",\n        temperature=0.7,\n        stream=True,\n        stream_callback=stream_callback\n    )\n\n    # Process the stream\n    async for chunk in stream:\n        # The callback already handles printing\n        pass\n</code></pre>"},{"location":"developer/mcp/#using-with-chainlit","title":"Using with Chainlit","text":"<p>VT.ai includes a dedicated handler for using MCP with Chainlit:</p> <pre><code>import chainlit as cl\nfrom vtai.utils.mcp_integration import ChainlitMCPHandler, initialize_mcp\n\n# Initialize MCP handler\nmcp_handler = ChainlitMCPHandler()\n\n@cl.on_message\nasync def on_message(message: cl.Message):\n    # Get message history\n    message_history = cl.user_session.get(\"message_history\", [])\n\n    # Add user message\n    message_history.append({\"role\": \"user\", \"content\": message.content})\n\n    # Create response message\n    response_message = cl.Message(content=\"\")\n    await response_message.send()\n\n    # Handle with MCP\n    response_text = await mcp_handler.handle_message(\n        message_history=message_history,\n        current_message=response_message,\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    # Add to history\n    message_history.append({\"role\": \"assistant\", \"content\": response_text})\n    cl.user_session.set(\"message_history\", message_history)\n</code></pre>"},{"location":"developer/mcp/#configuring-mcp","title":"Configuring MCP","text":"<p>You can configure MCP by modifying the settings in <code>vtai/utils/mcp_config.py</code> or by setting environment variables:</p> <ul> <li><code>MCP_HOST</code>: Host address for the MCP server (default: \"localhost\")</li> <li><code>MCP_PORT</code>: Port for the MCP server (default: 9393)</li> <li><code>MCP_DEFAULT_MODEL</code>: Default model--mcp-port to use when none is specified (default: \"o3-mini\")</li> </ul> <p>Model mappings are defined in <code>MCP_MODEL_MAP</code> in the config file.</p>"},{"location":"developer/mcp/#example-application","title":"Example Application","text":"<p>VT.ai includes a complete demo application that showcases MCP integration with Chainlit. You can run it with:</p> <pre><code>chainlit run examples/mcp_demo.py\n</code></pre> <p>This demo shows:</p> <ul> <li>Model switching without changing application code</li> <li>Streaming responses</li> <li>Temperature adjustment</li> <li>Token count display</li> </ul>"},{"location":"developer/mcp/#advanced-usage","title":"Advanced Usage","text":""},{"location":"developer/mcp/#custom-model-mappings","title":"Custom Model Mappings","text":"<p>You can create custom model mappings:</p> <pre><code>from vtai.utils.mcp_integration import initialize_mcp\n\ncustom_model_map = {\n    \"my-fast-model\": \"o3-mini\",\n    \"my-smart-model\": \"o1\",\n    \"my-creative-model\": \"claude-3-opus-20240229\"\n}\n\nmcp_config = initialize_mcp(model_map=custom_model_map)\n</code></pre>"},{"location":"developer/mcp/#direct-api-calls","title":"Direct API Calls","text":"<p>You can call the MCP API directly:</p> <pre><code>from vtai.utils.mcp_integration import call_mcp_api\n\nasync def direct_call():\n    messages = [\n        {\"role\": \"user\", \"content\": \"Hello, world!\"}\n    ]\n\n    response = await call_mcp_api(\n        messages=messages,\n        model=\"gpt-4o-mini\",\n        temperature=0.7\n    )\n\n    print(response)\n</code></pre>"},{"location":"developer/mcp/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you encounter errors connecting to the MCP server, check that it's running by looking for the log message \"Started MCP server on localhost:9393\"</li> <li>If a model isn't working, verify that you've set the appropriate API key for that provider</li> <li>For model mapping issues, check the <code>MCP_MODEL_MAP</code> in <code>mcp_config.py</code></li> </ul> <p>For more information on the Model Context Protocol, visit the MCP documentation.</p>"},{"location":"developer/models/","title":"Models Integration","text":"<p>This guide explains how VT.ai integrates with different AI model providers and how to work with the models API.</p>"},{"location":"developer/models/#architecture-overview","title":"Architecture Overview","text":"<p>VT.ai uses LiteLLM as a unified interface to multiple AI providers, which provides:</p> <ul> <li>A consistent API across different models</li> <li>Automatic fallbacks and retries</li> <li>Standardized error handling</li> <li>Easy switching between models</li> </ul>"},{"location":"developer/models/#provider-integration","title":"Provider Integration","text":""},{"location":"developer/models/#built-in-providers","title":"Built-in Providers","text":"<p>VT.ai comes with built-in support for several providers:</p> <ul> <li>OpenAI (GPT-o1, GPT-o3, GPT-4o)</li> <li>Anthropic (Claude models)</li> <li>Google (Gemini models)</li> <li>Local models via Ollama</li> <li>And others (DeepSeek, Cohere, etc.)</li> </ul>"},{"location":"developer/models/#provider-configuration","title":"Provider Configuration","text":"<p>Provider configuration is managed in <code>vtai/utils/llm_providers_config.py</code>, where:</p> <ul> <li>Model-to-provider mappings are defined</li> <li>Environment variable names for API keys are specified</li> <li>Default parameters for each model are set</li> <li>Icons and display names are configured</li> </ul>"},{"location":"developer/models/#working-with-models","title":"Working with Models","text":""},{"location":"developer/models/#model-selection","title":"Model Selection","text":"<p>Models are selected through several mechanisms:</p> <ol> <li>User Selection: Via UI or command line</li> <li>Semantic Router: Automatically based on query</li> <li>Specialized Handlers: For specific tasks (vision, image generation)</li> </ol>"},{"location":"developer/models/#model-configuration","title":"Model Configuration","text":"<p>Models can be configured with parameters like:</p> <pre><code># Example configuration\nmodel_params = {\n    \"model\": \"o3-mini\",  # OpenAI GPT-o3 Mini model\n    \"temperature\": 0.7,  # Controls randomness\n    \"top_p\": 0.9,        # Controls diversity\n    \"max_tokens\": 1000   # Maximum output length\n}\n</code></pre>"},{"location":"developer/models/#calling-models","title":"Calling Models","text":"<p>VT.ai uses LiteLLM's completion interface for consistency:</p> <pre><code># Example asynchronous call using LiteLLM\nfrom litellm import acompletion\n\nasync def call_model(messages, model=\"o3-mini\", **kwargs):\n    try:\n        response = await acompletion(\n            model=model,\n            messages=messages,\n            **kwargs\n        )\n        return response\n    except Exception as e:\n        # Error handling\n        raise\n</code></pre>"},{"location":"developer/models/#specialized-model-usage","title":"Specialized Model Usage","text":""},{"location":"developer/models/#vision-models","title":"Vision Models","text":"<p>Vision models require special handling for image inputs:</p> <pre><code># Example vision model call\nasync def call_vision_model(image_url, prompt, model=\"4o\", **kwargs):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n            ]\n        }\n    ]\n\n    response = await acompletion(\n        model=model,\n        messages=messages,\n        **kwargs\n    )\n    return response\n</code></pre>"},{"location":"developer/models/#tts-models","title":"TTS Models","text":"<p>Text-to-speech models handle audio generation:</p> <pre><code># Example TTS call\nasync def generate_speech(text, model=\"tts-1\", voice=\"alloy\"):\n    from litellm import atts\n\n    try:\n        response = await atts(\n            text=text,\n            model=model,\n            voice=voice\n        )\n        return response\n    except Exception as e:\n        # Error handling\n        raise\n</code></pre>"},{"location":"developer/models/#image-generation-models","title":"Image Generation Models","text":"<p>VT.ai supports advanced image generation capabilities with multiple models:</p> <pre><code># Example image generation call\nasync def generate_image(prompt, **kwargs):\n    # Configure settings for image generation\n    image_size = settings.get(SETTINGS_IMAGE_GEN_IMAGE_SIZE)  # 1024x1024, 1536x1024, etc.\n    image_quality = settings.get(SETTINGS_IMAGE_GEN_IMAGE_QUALITY)  # \"standard\", \"high\"\n    background = settings.get(SETTINGS_IMAGE_GEN_BACKGROUND)  # \"auto\", \"transparent\"\n    output_format = settings.get(SETTINGS_IMAGE_GEN_OUTPUT_FORMAT)  # \"png\", \"jpeg\"\n    compression = settings.get(SETTINGS_IMAGE_GEN_OUTPUT_COMPRESSION)  # 0-100\n\n    # GPT-Image-1 is now the default image generation model\n    response = await client.images.generate(\n        model=\"gpt-image-1\",\n        prompt=prompt,\n        n=1,\n        size=image_size,\n        quality=image_quality,\n        background=background,\n        output_format=output_format,\n        output_compression=compression,\n        **kwargs\n    )\n    return response\n</code></pre>"},{"location":"developer/models/#gpt-image-1-configuration","title":"GPT-Image-1 Configuration","text":"<p>GPT-Image-1 supports several configuration options in VT.ai:</p> <ul> <li> <p>Image Size: Control the dimensions of generated images</p> <ul> <li><code>1024x1024</code> (square - default)</li> <li><code>1536x1024</code> (landscape)</li> <li><code>1024x1536</code> (portrait)</li> </ul> </li> <li> <p>Image Quality: Control the rendering quality</p> <ul> <li><code>standard</code> - Regular quality (default)</li> <li><code>high</code> - Enhanced quality for detailed images</li> </ul> </li> <li> <p>Background Type: Control transparency</p> <ul> <li><code>auto</code> - Let the model decide (default)</li> <li><code>transparent</code> - Create images with transparent backgrounds (for PNG format)</li> <li><code>opaque</code> - Force an opaque background</li> </ul> </li> <li> <p>Output Format: Select image format</p> <ul> <li><code>jpeg</code> - Good for photographs (default)</li> <li><code>png</code> - Best for images needing transparency</li> <li><code>webp</code> - Optimized for web use with good compression</li> </ul> </li> <li> <p>Moderation Level: Content filtering level</p> <ul> <li><code>auto</code> - Standard moderation (default)</li> <li><code>low</code> - Less restrictive moderation</li> </ul> </li> <li> <p>Compression Quality: For JPEG and WebP formats</p> <ul> <li>Values from 0-100 (75 is default)</li> <li>Higher values produce better quality but larger files</li> </ul> </li> </ul> <p>All these settings can be configured through environment variables:</p> <pre><code># Example configuration\nexport VT_SETTINGS_IMAGE_GEN_IMAGE_SIZE=\"1536x1024\"\nexport VT_SETTINGS_IMAGE_GEN_IMAGE_QUALITY=\"high\"\nexport VT_SETTINGS_IMAGE_GEN_BACKGROUND=\"transparent\"\nexport VT_SETTINGS_IMAGE_GEN_OUTPUT_FORMAT=\"png\"\nexport VT_SETTINGS_IMAGE_GEN_OUTPUT_COMPRESSION=\"90\"\nexport VT_SETTINGS_IMAGE_GEN_MODERATION=\"auto\"\n</code></pre>"},{"location":"developer/models/#error-handling","title":"Error Handling","text":"<p>VT.ai implements robust error handling for model calls:</p> <ul> <li>API rate limiting errors</li> <li>Authentication errors</li> <li>Model-specific errors</li> <li>Network errors</li> </ul> <p>The main error handling is centralized in <code>vtai/utils/error_handlers.py</code>.</p>"},{"location":"developer/models/#model-performance","title":"Model Performance","text":""},{"location":"developer/models/#streaming-responses","title":"Streaming Responses","text":"<p>VT.ai supports streaming responses for a better user experience:</p> <pre><code># Example streaming call\nasync def stream_model_response(messages, model=\"o3-mini\", **kwargs):\n    from litellm import acompletion\n\n    response_stream = await acompletion(\n        model=model,\n        messages=messages,\n        stream=True,\n        **kwargs\n    )\n\n    collected_content = \"\"\n    async for chunk in response_stream:\n        content = chunk.choices[0].delta.content\n        if content:\n            collected_content += content\n            # Handle chunk processing\n\n    return collected_content\n</code></pre>"},{"location":"developer/models/#caching","title":"Caching","text":"<p>VT.ai implements caching for model responses to improve performance and reduce API costs:</p> <ul> <li>In-memory cache for short-term use</li> <li>Disk-based cache for persistent storage</li> </ul>"},{"location":"developer/models/#adding-new-model-providers","title":"Adding New Model Providers","text":"<p>To add support for a new model provider:</p> <ol> <li>Update the provider configuration in <code>llm_providers_config.py</code></li> <li>Add the appropriate API key handling</li> <li>Test compatibility with the semantic router</li> <li>Implement any specialized handling if needed</li> </ol> <p>See the Extending VT.ai guide for more details.</p> <p>This page is under construction. More detailed information about model integration will be added soon.</p>"},{"location":"developer/routing/","title":"Semantic Routing in VT.ai","text":"<p>This document explains VT.ai's semantic routing system, which intelligently directs user queries to specialized handlers based on their intent.</p>"},{"location":"developer/routing/#overview","title":"Overview","text":"<p>The semantic routing system is a key component of VT.ai that analyzes user queries and automatically determines the most appropriate handler to process them. Unlike simple keyword matching, this system uses vector embeddings to understand the semantic meaning of queries, making it more robust and flexible.</p>"},{"location":"developer/routing/#how-it-works","title":"How It Works","text":"<p>The routing process follows these steps:</p> <ol> <li> <p>Query Embedding: The user's query is converted into a vector representation (embedding) using the BAAI/bge-small-en-v1.5 model via FastEmbed.</p> </li> <li> <p>Intent Matching: The query embedding is compared against predefined intent embeddings using cosine similarity to find the closest match.</p> </li> <li> <p>Handler Selection: Based on the matched intent, the system selects the appropriate handler function to process the query.</p> </li> <li> <p>Response Generation: The selected handler processes the query and generates a response, which may involve calling specific models or executing specialized logic.</p> </li> </ol>"},{"location":"developer/routing/#key-components","title":"Key Components","text":""},{"location":"developer/routing/#router-module","title":"Router Module","text":"<p>The router module is located in <code>vtai/router/</code> and consists of:</p> <ul> <li><code>__init__.py</code>: Core routing functionality</li> <li><code>constants.py</code>: Routing-related constants</li> <li><code>layers.json</code>: Intent definitions and examples</li> <li><code>trainer.py</code>: Utility for training the router with new intents</li> </ul>"},{"location":"developer/routing/#intent-definitions","title":"Intent Definitions","text":"<p>Intents are defined in the <code>layers.json</code> file with the following structure:</p> <pre><code>{\n  \"intents\": [\n    {\n      \"name\": \"intent_name\",\n      \"description\": \"Description of what this intent handles\",\n      \"examples\": [\n        \"Example query 1\",\n        \"Example query 2\",\n        \"Example query 3\"\n      ]\n    },\n    // More intents...\n  ]\n}\n</code></pre> <p>Each intent includes:</p> <ul> <li>A unique name</li> <li>A description of what it handles</li> <li>Example queries that match this intent</li> </ul>"},{"location":"developer/routing/#embedding-model","title":"Embedding Model","text":"<p>VT.ai uses the BAAI/bge-small-en-v1.5 embedding model through FastEmbed, which provides:</p> <ul> <li>High-quality semantic vector representations</li> <li>Efficient computation for low-latency routing</li> <li>Good performance across multiple languages</li> </ul>"},{"location":"developer/routing/#handler-functions","title":"Handler Functions","text":"<p>Handler functions are defined in <code>vtai/utils/conversation_handlers.py</code> and are connected to intents in the router configuration. Each handler:</p> <ul> <li>Takes the user message and conversation history as input</li> <li>Processes the query according to its specialized logic</li> <li>Generates an appropriate response</li> <li>Sends the response back to the user</li> </ul>"},{"location":"developer/routing/#default-intents","title":"Default Intents","text":"<p>VT.ai includes several predefined intents:</p> <ol> <li>General Conversation: For standard chat interactions</li> <li>Image Generation: For creating images from text descriptions</li> <li>Vision Analysis: For analyzing and interpreting images</li> <li>Thinking Mode: For accessing step-by-step reasoning from models</li> <li>Code Assistance: For programming help and code execution</li> <li>Data Analysis: For working with data and performing calculations</li> </ol>"},{"location":"developer/routing/#customizing-the-router","title":"Customizing the Router","text":"<p>The semantic router can be extended with new intents. The process involves:</p> <ol> <li>Adding Intent Definitions: Update <code>vtai/router/layers.json</code> with new intents and examples</li> <li>Training the Router: Run <code>python -m vtai.router.trainer</code> to update embeddings</li> <li>Creating Handler Functions: Implement specialized handlers in <code>vtai/utils/conversation_handlers.py</code></li> <li>Updating Router Configuration: Connect intents to handlers in the router initialization</li> </ol> <p>For detailed instructions, see the Extending VT.ai guide.</p>"},{"location":"developer/routing/#dynamic-routing-control","title":"Dynamic Routing Control","text":"<p>VT.ai allows users to control the routing behavior:</p> <ul> <li>Enable/Disable: Users can toggle dynamic routing in the settings menu</li> <li>Override: Users can select specific models to bypass routing</li> <li>Force Routing: Adding specific markers to messages can force certain handlers</li> </ul>"},{"location":"developer/routing/#performance-considerations","title":"Performance Considerations","text":"<p>The semantic router is designed to be efficient, but there are some considerations:</p> <ul> <li>Embedding Computation: The initial embedding of intents happens at startup</li> <li>Query Embedding: Each user query needs to be embedded before routing</li> <li>Model Loading: The embedding model is loaded into memory at startup</li> <li>Cache Usage: Frequent queries may benefit from embedding caching</li> </ul>"},{"location":"developer/routing/#technical-details","title":"Technical Details","text":""},{"location":"developer/routing/#embedding-process","title":"Embedding Process","text":"<p>The technical process of embedding a query involves:</p> <pre><code># Pseudocode for query embedding\nfrom fastembed import TextEmbedding\n\n# Load the model (done at initialization)\nembedding_model = TextEmbedding(\"BAAI/bge-small-en-v1.5\")\n\n# Embed the query\nquery_embedding = embedding_model.embed(query)\n\n# Compare to intent embeddings\nsimilarities = [cosine_similarity(query_embedding, intent_embedding)\n                for intent_embedding in intent_embeddings]\n\n# Get the best match\nbest_match_index = np.argmax(similarities)\nbest_intent = intents[best_match_index]\n</code></pre>"},{"location":"developer/routing/#routing-decision-logic","title":"Routing Decision Logic","text":"<p>The routing decision is made based on:</p> <ol> <li>The closest matching intent</li> <li>A confidence threshold to avoid mis-routing</li> <li>User preferences and settings</li> <li>Fallback logic for when no clear match is found</li> </ol>"},{"location":"developer/routing/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with the routing system:</p> <ul> <li>Misrouted Queries: Add more examples to the relevant intent</li> <li>Unhandled Intents: Check if you need to create a new intent</li> <li>Slow Routing: Ensure embeddings are properly cached</li> <li>Failed Routing: Verify the embedding model is correctly loaded</li> </ul>"},{"location":"developer/tools/","title":"Assistant Tools","text":"<p>This guide explains the Assistant Tools feature in VT.ai, which provides specialized capabilities beyond standard chat interaction.</p>"},{"location":"developer/tools/#overview","title":"Overview","text":"<p>VT.ai includes an Assistant mode based on OpenAI's Assistants API that provides powerful tools for code interpretation, file handling, and function calling. These tools enable more complex workflows and specialized functionality.</p>"},{"location":"developer/tools/#available-tools","title":"Available Tools","text":""},{"location":"developer/tools/#code-interpreter","title":"Code Interpreter","text":"<p>The Code Interpreter tool allows executing Python code directly within the chat interface:</p> <ul> <li>Features:</li> <li>Run Python code in a sandboxed environment</li> <li>Create charts and visualizations</li> <li>Perform data analysis</li> <li>Execute mathematical computations</li> <li> <p>Generate plots and figures</p> </li> <li> <p>Usage:</p> </li> </ul> <pre><code># Sample code execution in Assistant mode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.title(\"Sine Wave\")\nplt.xlabel(\"X\")\nplt.ylabel(\"sin(x)\")\nplt.show()\n</code></pre>"},{"location":"developer/tools/#file-processing","title":"File Processing","text":"<p>The File Processing tool enables working with uploaded files:</p> <ul> <li>Supported File Types:</li> <li>Text files (<code>.txt</code>, <code>.md</code>, etc.)</li> <li>CSV and spreadsheets (<code>.csv</code>, <code>.xlsx</code>)</li> <li>Images (<code>.jpg</code>, <code>.png</code>, etc.)</li> <li>PDFs (<code>.pdf</code>)</li> <li> <p>Code files (<code>.py</code>, <code>.js</code>, etc.)</p> </li> <li> <p>Capabilities:</p> </li> <li>Extract text from documents</li> <li>Analyze images</li> <li>Process structured data</li> <li>Generate insights from file content</li> </ul>"},{"location":"developer/tools/#function-calling","title":"Function Calling","text":"<p>Function Calling allows the assistant to interact with external systems and APIs:</p> <ul> <li>Current Status: Function tools are temporarily disabled in the current version.</li> <li>Planned Features:</li> <li>Web search integration</li> <li>External API calls</li> <li>Database interactions</li> <li>System operations</li> </ul>"},{"location":"developer/tools/#implementation-details","title":"Implementation Details","text":""},{"location":"developer/tools/#assistant-configuration","title":"Assistant Configuration","text":"<p>The Assistant configuration is defined in the codebase:</p> <pre><code># Assistant configuration pseudocode\nassistant = {\n    \"name\": \"VT.ai Code Assistant\",\n    \"description\": \"A helpful code and data analysis assistant\",\n    \"instructions\": \"You are a helpful assistant that can execute code...\",\n    \"tools\": [\n        {\"type\": \"code_interpreter\"},\n        {\"type\": \"retrieval\"},\n        # Function tools temporarily disabled\n    ],\n    \"model\": \"o3\"\n}\n</code></pre>"},{"location":"developer/tools/#tool-processing-flow","title":"Tool Processing Flow","text":"<p>The processing flow for tools follows this pattern:</p> <ol> <li>User submits a query in Assistant mode</li> <li>Query is processed by the appropriate model</li> <li>If the model decides to use a tool, a tool call is generated</li> <li>VT.ai processes the tool call (e.g., executes code, analyzes files)</li> <li>Tool output is returned to the model</li> <li>Model generates final response incorporating tool results</li> </ol>"},{"location":"developer/tools/#processing-tool-calls","title":"Processing Tool Calls","text":"<p>Tool calls are processed by specialized handlers:</p> <pre><code># Example of code interpreter tool processing\nasync def process_code_interpreter_tool(\n    step_references: Dict[str, cl.Step], step: Any, tool_call: Any\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process code interpreter tool calls.\n\n    Args:\n        step_references: Dictionary of step references\n        step: The run step\n        tool_call: The tool call to process\n\n    Returns:\n        Tool output dictionary\n    \"\"\"\n    output_value = \"\"\n    if tool_call.code_interpreter.outputs and len(tool_call.code_interpreter.outputs) &gt; 0:\n        output_value = tool_call.code_interpreter.outputs[0]\n\n    # Process and display the results in the UI\n    await process_tool_call(\n        step_references=step_references,\n        step=step,\n        tool_call=tool_call,\n        name=tool_call.type,\n        input=tool_call.code_interpreter.input or \"# Generating code\",\n        output=output_value,\n        show_input=\"python\",\n    )\n\n    return {\n        \"output\": tool_call.code_interpreter.outputs or \"\",\n        \"tool_call_id\": tool_call.id,\n    }\n</code></pre>"},{"location":"developer/tools/#thread-management","title":"Thread Management","text":"<p>Assistant mode uses threads to maintain conversation context:</p> <ul> <li>Thread Creation: A new thread is created at the start of each Assistant mode session</li> <li>Message Storage: Messages are stored in the thread for context</li> <li>Run Management: Each user query creates a \"run\" instance that processes the query</li> <li>Step Tracking: Individual steps within a run are tracked and displayed</li> </ul>"},{"location":"developer/tools/#user-interface","title":"User Interface","text":"<p>The Assistant Tools interface in VT.ai provides:</p> <ul> <li>Step Visualization: Shows each step of the assistant's work</li> <li>Code Displays: Properly formatted code blocks with syntax highlighting</li> <li>Output Visualization: Displays images, charts, and other outputs</li> <li>Interactive Elements: Allows users to interact with the assistant's outputs</li> </ul>"},{"location":"developer/tools/#extending-assistant-tools","title":"Extending Assistant Tools","text":"<p>While function tools are temporarily disabled, VT.ai is designed to be extended with custom tools:</p> <ol> <li>Define the tool interface in <code>vtai/tools/</code></li> <li>Add the tool to the assistant configuration</li> <li>Implement the tool processing logic</li> <li>Add UI components to display tool outputs</li> </ol> <p>See the Extending VT.ai guide for more details.</p>"},{"location":"developer/tools/#best-practices","title":"Best Practices","text":"<p>When using Assistant Tools:</p> <ol> <li>Code Interpreter:</li> <li>Keep code snippets focused on a single task</li> <li>For data analysis, provide clear column descriptions</li> <li> <p>Use visualization when appropriate</p> </li> <li> <p>File Handling:</p> </li> <li>Provide context about uploaded files</li> <li>Ask specific questions about file content</li> <li> <p>Upload files in appropriate formats</p> </li> <li> <p>General Usage:</p> </li> <li>Be explicit about what you want the assistant to do</li> <li>Check intermediate results</li> <li>Break complex tasks into smaller steps</li> </ol> <p>This page is under construction. More detailed information about assistant tools will be added soon.</p>"},{"location":"user/configuration/","title":"Configuration","text":"<p>This page provides information about configuring VT.ai to suit your needs.</p>"},{"location":"user/configuration/#configuration-options","title":"Configuration Options","text":"<p>VT.ai offers several configuration options that can be set through various methods:</p> <ol> <li>Command-line arguments</li> <li>Environment variables</li> <li>UI settings</li> <li>Configuration files</li> </ol>"},{"location":"user/configuration/#model-configuration","title":"Model Configuration","text":"<p>You can configure which models to use for different types of operations:</p> <ul> <li>Chat models</li> <li>Vision models</li> <li>Text-to-speech models</li> <li>Image generation models</li> </ul>"},{"location":"user/configuration/#web-search-settings","title":"Web Search Settings","text":"<p>Customize how VT.ai handles web searches:</p> <ul> <li>Summarize Search Results: Toggle between AI-summarized results and raw search output</li> <li>Enabled (default): Synthesizes multiple search results into a coherent summary</li> <li> <p>Disabled: Shows raw search results with minimal processing</p> </li> <li> <p>Include URLs: Choose whether to include source URLs in the search results</p> </li> <li> <p>Useful for citation and verification</p> </li> <li> <p>Search Context Size: Control the depth of web searches</p> </li> <li>Options: low, medium, high</li> <li>Higher values search more deeply but may take longer</li> </ul>"},{"location":"user/configuration/#performance-settings","title":"Performance Settings","text":"<p>Fine-tune the performance of your models:</p> <ul> <li>Temperature</li> <li>Top-P</li> <li>Max tokens</li> <li>Response format</li> </ul>"},{"location":"user/configuration/#routing-configuration","title":"Routing Configuration","text":"<p>Configure the semantic routing system:</p> <ul> <li>Enable/disable dynamic routing</li> <li>Customize routing thresholds</li> <li>Add custom intents</li> </ul>"},{"location":"user/configuration/#api-key-management","title":"API Key Management","text":"<p>Manage your API keys for different providers:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Google</li> <li>Other providers</li> </ul>"},{"location":"user/configuration/#ui-customization","title":"UI Customization","text":"<p>Customize the user interface to your preferences:</p> <ul> <li>Theme settings</li> <li>Display options</li> <li>Keyboard shortcuts</li> </ul> <p>This page is under construction. More detailed configuration information will be added soon.</p>"},{"location":"user/features/","title":"VT.ai Features","text":"<p>This page provides detailed information about VT.ai's key features and how to use them effectively.</p>"},{"location":"user/features/#chat-modes","title":"Chat Modes","text":"<p>VT.ai offers different chat modes to suit your specific needs:</p>"},{"location":"user/features/#standard-chat","title":"Standard Chat","text":"<p>The standard chat mode provides access to all configured LLM providers with dynamic conversation routing:</p> <ul> <li>Automatic classification and routing of queries to appropriate models</li> <li>Support for text, image, and audio inputs</li> <li>Full access to all VT.ai features</li> </ul> <p>To use standard chat:</p> <ol> <li>Simply type your message in the input field</li> <li>Press Enter to send</li> <li>The system will automatically route your query to the most appropriate model</li> </ol>"},{"location":"user/features/#assistant-mode-beta","title":"Assistant Mode (Beta)","text":"<p>Assistant mode provides specialized capabilities for more complex tasks:</p> <ul> <li>Code interpreter for executing Python code</li> <li>File attachment support (PDF, CSV, images, etc.)</li> <li>Persistent conversation threads</li> <li>Function calling for external integrations</li> </ul> <p>To use assistant mode:</p> <ol> <li>Switch to the Assistant profile in the dropdown menu</li> <li>Upload files if needed</li> <li>Type your queries as normal</li> <li>View the step-by-step execution in the interface</li> </ol>"},{"location":"user/features/#specialized-features","title":"Specialized Features","text":""},{"location":"user/features/#web-search-with-smart-summarization","title":"Web Search with Smart Summarization","text":"<p>VT.ai can search the web for information and intelligently summarize the results:</p> <ul> <li>Accumulate information from multiple search results</li> <li>Generate coherent, comprehensive summaries</li> <li>Cite sources with proper attribution</li> <li>Toggle between raw results and AI-synthesized summaries</li> </ul> <p>To use web search with summarization:</p> <ol> <li>Ask a question that might require current information</li> <li>VT.ai will automatically route to the web search tool</li> <li>Results will be summarized into a concise, readable answer</li> <li>Sources will be listed with clickable links</li> </ol> <p>You can control summarization behavior:</p> <ul> <li>Enable/disable summarization in the settings menu</li> <li>When enabled, multiple search results are synthesized into a unified response</li> <li>When disabled, search results are presented in a more raw format</li> </ul> <p>Example queries:</p> <ul> <li>\"What are the latest developments in quantum computing?\"</li> <li>\"Search for information about sustainable energy solutions\"</li> <li>\"Find recent news about Mars exploration\"</li> </ul>"},{"location":"user/features/#thinking-mode","title":"Thinking Mode","text":"<p>Thinking mode gives you access to step-by-step reasoning from the models:</p> <ul> <li>See the model's internal reasoning process</li> <li>Understand how the model arrived at its conclusion</li> <li>Great for learning, debugging, and complex problem-solving</li> </ul> <p>To use thinking mode:</p> <ol> <li>Add the <code>&lt;think&gt;</code> tag at the beginning of your message</li> <li>The model will show its reasoning steps before providing the final answer</li> <li>You can see both the thinking process and the final response</li> </ol> <p>Example:</p> <pre><code>&lt;think&gt;What are the key factors that contributed to the Industrial Revolution?\n</code></pre>"},{"location":"user/features/#image-analysis","title":"Image Analysis","text":"<p>VT.ai can analyze and interpret images:</p> <ul> <li>Upload images directly from your device</li> <li>Provide URLs to online images</li> <li>Get detailed descriptions and analysis</li> </ul> <p>To analyze an image:</p> <ol> <li>Click the upload button or paste an image URL</li> <li>Ask a question about the image</li> <li>The system will analyze the image and respond to your query</li> </ol> <p>Example queries:</p> <ul> <li>\"What's in this image?\"</li> <li>\"Can you describe this diagram?\"</li> <li>\"What text appears in this screenshot?\"</li> </ul>"},{"location":"user/features/#image-generation","title":"Image Generation","text":"<p>Generate images based on text descriptions:</p> <ul> <li>DALL-E 3: Create custom images from detailed prompts</li> <li>GPT-Image-1: Advanced image generation with extensive customization options</li> <li>Transparent backgrounds for logos and graphics</li> <li>Multiple output formats (PNG, JPEG)</li> <li>Customizable dimensions (square, landscape, portrait)</li> <li>Variable quality and compression settings</li> <li>Moderation controls</li> </ul> <p>To generate an image:</p> <ol> <li>Type a prompt like \"Generate an image of a futuristic city with flying cars\"</li> <li>The system will recognize the image generation intent</li> <li>The appropriate image generation model will create and display the image based on your description</li> </ol> <p>For advanced GPT-Image-1 options, you can configure:</p> <ul> <li>Image size: Set with <code>VT_SETTINGS_IMAGE_GEN_IMAGE_SIZE</code> (e.g., \"1024x1024\", \"1536x1024\", \"1024x1536\")</li> <li>Quality: Set with <code>VT_SETTINGS_IMAGE_GEN_IMAGE_QUALITY</code> (e.g., \"standard\", \"high\")</li> <li>Background: Set with <code>VT_SETTINGS_IMAGE_GEN_BACKGROUND</code> (e.g., \"auto\", \"transparent\")</li> <li>Output format: Set with <code>VT_SETTINGS_IMAGE_GEN_OUTPUT_FORMAT</code> (e.g., \"png\", \"jpeg\")</li> <li>Compression: Set with <code>VT_SETTINGS_IMAGE_GEN_OUTPUT_COMPRESSION</code> (e.g., \"90\", \"100\")</li> </ul>"},{"location":"user/features/#voice-interaction","title":"Voice Interaction","text":"<p>VT.ai supports voice-based interaction:</p> <ul> <li>Text-to-speech for hearing responses</li> <li>Multiple voice models to choose from</li> <li>Natural speech synthesis</li> </ul> <p>To use voice features:</p> <ol> <li>Enable TTS in the settings menu</li> <li>Select your preferred voice model</li> <li>Each response will include a speech button to listen to the content</li> </ol>"},{"location":"user/features/#model-selection","title":"Model Selection","text":"<p>VT.ai supports a wide range of models:</p> <ul> <li>OpenAI: GPT-o1, GPT-o3 Mini, GPT-4o</li> <li>Anthropic: Claude 3.5/3.7 (Sonnet, Opus)</li> <li>Google: Gemini 2.0/2.5</li> <li>Vision Models: GPT-4o, Gemini 1.5 Pro/Flash, Llama3.2 Vision</li> <li>TTS Models: GPT-4o mini TTS, TTS-1, TTS-1-HD</li> <li>Local Models: Llama3, Mistral, DeepSeek R1 (via Ollama)</li> </ul> <p>You can select models in several ways:</p> <ol> <li>Use the model selector in the settings menu</li> <li>Specify a model at startup with <code>vtai --model model-name</code></li> <li>Let the semantic router automatically select the best model for your query</li> </ol>"},{"location":"user/features/#configuration-options","title":"Configuration Options","text":"<p>VT.ai offers various configuration options accessible through the settings menu:</p> <ul> <li>Temperature: Control randomness in responses</li> <li>Top P: Adjust response diversity</li> <li>Image Generation Settings: Style and quality options</li> <li>TTS Settings: Voice models and quality options</li> <li>Routing Options: Enable/disable dynamic conversation routing</li> </ul> <p>For more details on configuration, see the Configuration page.</p>"},{"location":"user/getting-started/","title":"Getting Started with VT.ai","text":"<p>This guide will help you get up and running with VT.ai quickly.</p>"},{"location":"user/getting-started/#implementation-options","title":"Implementation Options","text":"<p>VT.ai is available in two implementations:</p> <ol> <li>Python Implementation: The original implementation with full feature support</li> <li>Rust Implementation: A high-performance port focused on efficiency and reliability</li> </ol> <p>You can choose which implementation to use based on your needs. Both share similar configuration approaches and supported models.</p>"},{"location":"user/getting-started/#python-installation-options","title":"Python Installation Options","text":"<p>VT.ai can be installed and run in multiple ways depending on your needs:</p>"},{"location":"user/getting-started/#quick-install-from-pypi","title":"Quick Install from PyPI","text":"<pre><code># Install VT.ai from PyPI\npip install vtai\n</code></pre>"},{"location":"user/getting-started/#quick-start-with-uvx-no-installation","title":"Quick Start with uvx (No Installation)","text":"<p>If you have uv installed, you can try VT.ai without installing it permanently:</p> <pre><code># Set your API key in the environment\nexport OPENAI_API_KEY='sk-your-key-here'\n\n# Run VT.ai directly using uvx\nuvx vtai\n</code></pre> <p>This creates a temporary virtual environment just for this session. When you're done, nothing is left installed on your system.</p>"},{"location":"user/getting-started/#installation-with-uv","title":"Installation with uv","text":"<pre><code># If you need to install uv first\npython -m pip install uv\n\n# Install VT.ai with uv\nuv tool install --force --python python3.11 vtai@latest\n</code></pre>"},{"location":"user/getting-started/#installation-with-pipx","title":"Installation with pipx","text":"<pre><code># If you need to install pipx first\npython -m pip install pipx\n\n# Install VT.ai with pipx\npipx install vtai\n</code></pre>"},{"location":"user/getting-started/#development-install-from-source","title":"Development Install (from source)","text":"<pre><code># Clone repository\ngit clone https://github.com/vinhnx/VT.ai.git\ncd VT.ai\n\n# Setup environment using uv\nuv venv\nsource .venv/bin/activate  # Linux/Mac\n.venv\\Scripts\\activate     # Windows\n\n# Install dependencies\nuv pip install -e .\n</code></pre>"},{"location":"user/getting-started/#rust-implementation-setup","title":"Rust Implementation Setup","text":"<p>The Rust implementation provides an alternative high-performance version:</p>"},{"location":"user/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust toolchain (1.77.0 or newer recommended)</li> <li>API keys for at least one LLM provider</li> </ul>"},{"location":"user/getting-started/#installation-steps","title":"Installation Steps","text":"<pre><code># Clone repository if you haven't already\ngit clone https://github.com/vinhnx/VT.ai.git\ncd VT.ai/rust-vtai\n</code></pre>"},{"location":"user/getting-started/#quick-start-with-runsh-recommended","title":"Quick Start with run.sh (Recommended)","text":"<p>The Rust implementation includes a convenient shell script that handles building and running the application:</p> <pre><code># Run the application using the convenience script\n./run.sh\n\n# With API key and model selection\n./run.sh --api-key openai=sk-your-key-here --model o3-mini\n</code></pre> <p>The <code>run.sh</code> script: - Builds the application in release mode - Checks for API keys in environment variables - Runs the application with any provided command-line arguments - Displays helpful messages and warnings</p>"},{"location":"user/getting-started/#manual-build-and-run","title":"Manual Build and Run","text":"<p>If you prefer to build and run manually:</p> <pre><code># Build the application\ncargo build --release\n\n# Run the application\n./target/release/vtai\n</code></pre>"},{"location":"user/getting-started/#api-key-configuration","title":"API Key Configuration","text":"<p>You'll need at least one API key to use VT.ai effectively. You can set your API keys in several ways:</p>"},{"location":"user/getting-started/#python-implementation","title":"Python Implementation","text":""},{"location":"user/getting-started/#command-line-option","title":"Command Line Option","text":"<pre><code># Set OpenAI API key\nvtai --api-key openai=&lt;your-key&gt;\n</code></pre>"},{"location":"user/getting-started/#environment-variables","title":"Environment Variables","text":"<pre><code># For OpenAI (recommended for first-time users)\nexport OPENAI_API_KEY='sk-your-key-here'\nvtai\n\n# For Anthropic Claude models\nexport ANTHROPIC_API_KEY='sk-ant-your-key-here'\nvtai --model sonnet\n\n# For Google Gemini models\nexport GEMINI_API_KEY='your-key-here'\nvtai --model gemini-2.5\n</code></pre>"},{"location":"user/getting-started/#rust-implementation","title":"Rust Implementation","text":"<p>The Rust implementation uses a similar configuration approach:</p> <pre><code># Set OpenAI API key\n./target/release/vtai --api-key openai=&lt;your-key&gt;\n\n# Select a specific model\n./target/release/vtai --model o3-mini\n</code></pre> <p>API keys are saved to the same configuration directory as the Python implementation for consistency.</p>"},{"location":"user/getting-started/#first-run-experience","title":"First Run Experience","text":"<p>When you run VT.ai for the first time:</p> <ol> <li>The application will create a configuration directory at <code>~/.config/vtai/</code></li> <li>It will download necessary model files (tokenizers, embeddings, etc.)</li> <li>The web interface will open at http://localhost:8000</li> <li>If no API keys are configured, you'll be prompted to add them</li> </ol> <p>To ensure the best first-run experience:</p> <pre><code># Set at least one API key before running (OpenAI recommended for beginners)\nexport OPENAI_API_KEY='sk-your-key-here'\n\n# Run the application (Python implementation)\nvtai\n\n# Or for the Rust implementation\ncd rust-vtai\ncargo run --release\n</code></pre>"},{"location":"user/getting-started/#basic-usage","title":"Basic Usage","text":"<p>After starting VT.ai, you'll be presented with a chat interface. Here are some basic operations:</p> <ol> <li>Standard Chat: Type a message and press Enter to send it.</li> <li>Image Analysis: Upload an image or provide a URL to analyze it.</li> <li>Image Generation: Type a prompt like \"Generate an image of a mountain landscape\" to create an image.</li> <li>Thinking Mode: Use the <code>&lt;think&gt;</code> tag to see the model's step-by-step reasoning.</li> <li>Voice Interaction: Enable voice features to interact with speech.</li> </ol> <p>For more detailed usage instructions, see the Features page.</p>"},{"location":"user/getting-started/#upgrading-vtai","title":"Upgrading VT.ai","text":""},{"location":"user/getting-started/#upgrading-the-python-implementation","title":"Upgrading the Python Implementation","text":"<p>To upgrade VT.ai to the latest version:</p> <pre><code># If installed with pip\npip install --upgrade vtai\n\n# If installed with pipx\npipx upgrade vtai\n\n# If installed with uv\nuv tool upgrade vtai\n</code></pre>"},{"location":"user/getting-started/#upgrading-the-rust-implementation","title":"Upgrading the Rust Implementation","text":"<p>To upgrade the Rust implementation:</p> <pre><code># Navigate to the repository\ncd VT.ai\n\n# Pull the latest changes\ngit pull\n\n# Build the latest version\ncd rust-vtai\ncargo build --release\n</code></pre>"},{"location":"user/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Features documentation to learn about all capabilities</li> <li>Learn about Configuration options</li> <li>Check out the Models documentation to understand different model options</li> <li>Visit Troubleshooting if you encounter any issues</li> <li>Review the Architecture documentation to understand how VT.ai works internally</li> </ul>"},{"location":"user/models/","title":"Models","text":"<p>This page provides information about the AI models supported by VT.ai and how to use them effectively.</p>"},{"location":"user/models/#supported-models","title":"Supported Models","text":"<p>VT.ai integrates with multiple AI providers and supports a wide range of models:</p>"},{"location":"user/models/#openai-models","title":"OpenAI Models","text":"<ul> <li>GPT-o1 (o1): High-performance general purpose model</li> <li>GPT-o3 Mini (o3-mini): Compact, efficient model for everyday tasks</li> <li>GPT-4o (4o): Advanced vision and multimodal capabilities</li> </ul>"},{"location":"user/models/#anthropic-models","title":"Anthropic Models","text":"<ul> <li>Claude 3.5 Sonnet (c3.5-sonnet): Balanced performance and efficiency</li> <li>Claude 3.7 Sonnet (sonnet): Advanced reasoning capabilities</li> <li>Claude 3.5 Haiku (c3.5-haiku): Fast, efficient model for common tasks</li> <li>Claude 3 Opus (opus): Highest capability model for complex tasks</li> </ul>"},{"location":"user/models/#google-models","title":"Google Models","text":"<ul> <li>Gemini 1.5 Pro (gemini-1.5-pro): Advanced multimodal capabilities</li> <li>Gemini 1.5 Flash (gemini-1.5-flash): Fast, efficient model</li> <li>Gemini 2.0 (gemini-2.0): Advanced reasoning model</li> </ul>"},{"location":"user/models/#other-providers","title":"Other Providers","text":"<ul> <li>DeepSeek (deepseek): High-quality alternative models</li> <li>Cohere (cohere): Specialized language models</li> <li>Mistral (mistral): Efficient open models</li> <li>Llama 3 (llama3): Meta's advanced open model</li> <li>Local Models (via Ollama): Run models locally for privacy and offline use</li> </ul>"},{"location":"user/models/#model-selection","title":"Model Selection","text":"<p>You can select models in several ways:</p> <ol> <li>Command Line:</li> </ol> <pre><code>vtai --model sonnet\n</code></pre> <ol> <li>UI Settings:</li> <li>Use the model selector in the settings menu</li> <li> <p>Change models during a conversation</p> </li> <li> <p>Dynamic Routing:</p> </li> <li>Allow VT.ai to automatically select the best model for your query</li> <li>Enable in settings with \"Use Dynamic Conversation Routing\"</li> </ol>"},{"location":"user/models/#model-capabilities","title":"Model Capabilities","text":"<p>Different models have different capabilities:</p>"},{"location":"user/models/#vision-capable-models","title":"Vision-Capable Models","text":"<p>For analyzing images and visual content:</p> <ul> <li>GPT-4o</li> <li>Gemini 1.5 Pro/Flash</li> <li>Claude 3 Sonnet/Opus</li> <li>Llama 3.2 Vision</li> </ul>"},{"location":"user/models/#tts-capable-models","title":"TTS-Capable Models","text":"<p>For text-to-speech generation:</p> <ul> <li>GPT-4o mini TTS</li> <li>TTS-1</li> <li>TTS-1-HD</li> </ul>"},{"location":"user/models/#image-generation-models","title":"Image Generation Models","text":"<p>For creating images from text descriptions:</p> <ul> <li>DALL-E 3: OpenAI's image generation model</li> <li>GPT-Image-1: Advanced image generation model with customizable settings</li> <li>Supports transparent backgrounds</li> <li>Multiple output formats (PNG, JPEG)</li> <li>Customizable dimensions (square, landscape, portrait)</li> <li>Quality settings (standard, high)</li> <li>Advanced compression options</li> </ul>"},{"location":"user/models/#performance-considerations","title":"Performance Considerations","text":"<p>When choosing models, consider these factors:</p> <ul> <li>Speed: Models like GPT-o3 Mini and Claude 3 Haiku are faster</li> <li>Quality: Models like GPT-o1, GPT-4o, and Claude 3 Opus offer higher quality</li> <li>Cost: Smaller models generally cost less to use</li> <li>Multimodal Needs: Only some models support image analysis</li> <li>Local Computation: Ollama models run locally but require more resources</li> </ul> <p>This page is under construction. More detailed model information will be added soon.</p>"},{"location":"user/troubleshooting/","title":"Troubleshooting","text":"<p>This page provides solutions to common issues you might encounter when using VT.ai.</p>"},{"location":"user/troubleshooting/#api-key-issues","title":"API Key Issues","text":""},{"location":"user/troubleshooting/#missing-api-keys","title":"Missing API Keys","text":"<p>Problem: VT.ai reports that API keys are missing or invalid.</p> <p>Solution:</p> <ol> <li>Check that you've set your API keys using one of these methods:</li> </ol> <pre><code># Command line\nvtai --api-key openai=sk-your-key\n\n# Environment variables\nexport OPENAI_API_KEY=sk-your-key\n</code></pre> <ol> <li>Verify that the API keys are valid by testing them directly with the provider</li> <li>Check the <code>~/.config/vtai/.env</code> file to ensure keys are saved correctly</li> </ol>"},{"location":"user/troubleshooting/#rate-limiting","title":"Rate Limiting","text":"<p>Problem: You're encountering rate limit errors from API providers.</p> <p>Solution:</p> <ol> <li>Switch to a different model or provider</li> <li>Wait a few minutes before trying again</li> <li>Check if your API key has usage restrictions</li> <li>Consider upgrading your API tier with the provider</li> </ol>"},{"location":"user/troubleshooting/#installation-problems","title":"Installation Problems","text":""},{"location":"user/troubleshooting/#package-conflicts","title":"Package Conflicts","text":"<p>Problem: Conflicts with existing Python packages during installation.</p> <p>Solution:</p> <ol> <li>Use a virtual environment:</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\nvenv\\Scripts\\activate     # Windows\npip install vtai\n</code></pre> <ol> <li>Try installing with uv instead:</li> </ol> <pre><code>uv tool install vtai\n</code></pre> <ol> <li>Check for conflicting packages with <code>pip list</code></li> </ol>"},{"location":"user/troubleshooting/#missing-dependencies","title":"Missing Dependencies","text":"<p>Problem: VT.ai reports missing dependencies.</p> <p>Solution:</p> <ol> <li>Reinstall with all dependencies:</li> </ol> <pre><code>pip install vtai[all]\n</code></pre> <ol> <li>Check if you have enough disk space</li> <li>Try updating pip: <code>pip install --upgrade pip</code></li> </ol>"},{"location":"user/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"user/troubleshooting/#slow-responses","title":"Slow Responses","text":"<p>Problem: VT.ai is taking a long time to respond.</p> <p>Solution:</p> <ol> <li>Switch to a faster model (e.g., GPT-o3 Mini, Claude 3 Haiku)</li> <li>Check your internet connection</li> <li>Reduce query complexity</li> <li>Disable dynamic routing if it's causing delays</li> </ol>"},{"location":"user/troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Problem: VT.ai is using excessive system memory.</p> <p>Solution:</p> <ol> <li>Restart the application</li> <li>Use smaller models</li> <li>Close other memory-intensive applications</li> <li>Increase swap space if possible</li> </ol>"},{"location":"user/troubleshooting/#feature-specific-issues","title":"Feature-Specific Issues","text":""},{"location":"user/troubleshooting/#image-generation-not-working","title":"Image Generation Not Working","text":"<p>Problem: Image generation commands don't create images.</p> <p>Solution:</p> <ol> <li>Ensure you have a valid OpenAI API key with DALL-E access</li> <li>Try more explicit prompts like \"Generate an image of...\"</li> <li>Check for error messages in the console output</li> <li>Verify that you have sufficient API credits for image generation</li> </ol>"},{"location":"user/troubleshooting/#tts-not-working","title":"TTS Not Working","text":"<p>Problem: Text-to-speech functionality isn't working.</p> <p>Solution:</p> <ol> <li>Make sure TTS is enabled in settings</li> <li>Select a different TTS model</li> <li>Check audio output settings on your device</li> <li>Verify that you have the necessary API access for TTS features</li> </ol>"},{"location":"user/troubleshooting/#vision-analysis-issues","title":"Vision Analysis Issues","text":"<p>Problem: Image analysis doesn't work properly.</p> <p>Solution:</p> <ol> <li>Ensure you're using a vision-capable model like GPT-4o</li> <li>Check that the image format is supported (JPG, PNG, WebP)</li> <li>Try with a different image</li> <li>Make sure your query specifically references the image</li> </ol>"},{"location":"user/troubleshooting/#chainlit-interface-issues","title":"Chainlit Interface Issues","text":""},{"location":"user/troubleshooting/#ui-not-loading","title":"UI Not Loading","text":"<p>Problem: The web interface doesn't load properly.</p> <p>Solution:</p> <ol> <li>Check if the Chainlit server is running (look for log messages)</li> <li>Try a different web browser</li> <li>Clear your browser cache</li> <li>Check for port conflicts (default is 8000)</li> </ol>"},{"location":"user/troubleshooting/#settings-not-saving","title":"Settings Not Saving","text":"<p>Problem: Settings changes don't persist between sessions.</p> <p>Solution:</p> <ol> <li>Check that you have write permission to <code>~/.config/vtai/</code></li> <li>Manually edit settings in the configuration files</li> <li>Try running with elevated permissions if needed</li> </ol>"},{"location":"user/troubleshooting/#getting-more-help","title":"Getting More Help","text":"<p>If you're still experiencing issues:</p> <ol> <li>Check the GitHub repository for open issues</li> <li>Create a new issue with detailed information about your problem</li> <li>Join the community discussions for assistance</li> </ol> <p>This page is under construction. More troubleshooting information will be added soon.</p>"}]}